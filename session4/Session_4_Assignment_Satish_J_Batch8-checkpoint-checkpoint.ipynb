{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Session_4_Assignment_Satish_J_Batch8-checkpoint-checkpoint.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"bVUmHoTwNaOm","colab_type":"text"},"cell_type":"markdown","source":["### Satish J\n","### Batch 8"]},{"metadata":{"colab_type":"text","id":"BrqA9p3yK2pi"},"cell_type":"markdown","source":["### Imports"]},{"metadata":{"colab_type":"code","id":"11jkGSvz1OAN","colab":{}},"cell_type":"code","source":["import os\n","import tensorflow as tf\n","\n","from tqdm import tqdm\n","import numpy as np\n","np.random.seed(42)\n","\n","import keras\n","from keras.layers import Conv2D \n","from keras.layers import AveragePooling2D \n","from keras.layers import BatchNormalization \n","from keras.layers import Activation \n","from keras.layers import Dropout \n","from keras.regularizers import l2\n","from keras.initializers import VarianceScaling\n","from keras.layers import AveragePooling2D \n","from keras.layers import Concatenate, Input, Dense, GlobalAveragePooling2D\n","from keras.models import Model\n","from keras.datasets import cifar10\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import LearningRateScheduler,ModelCheckpoint\n","from keras.optimizers import SGD\n","from keras.utils import multi_gpu_model"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"OrY42bVGK8Nv"},"cell_type":"markdown","source":["### Building Network"]},{"metadata":{"colab_type":"text","id":"M19yq3mSLW2a"},"cell_type":"markdown","source":["#### Which Network ?\n","**DenseNet-BC(L=100, k=12)** ie network with **BottleNeck layers** and **Compression** is used,\n","\n","where,\n","- L = Depth / Number of Conv layers\n","- k = Growth rate\n","\n","- BottleNeck layers: involve following sequence of ops which helps to significantly reduce number of parameters in network.\n","    - Batch Normalization\n","    - Relu Activation\n","    - Conv(1x1)\n","- Compression: is one more way to reduce the number of output channels when data is flowing from one dense block to other. Compression is done in transition layer as shown below\n","    \n","    DenseBlock1----->Transition(compresssion)----->DenseBlock2\n","       "]},{"metadata":{"id":"slcB9F0pNaO1","colab_type":"text"},"cell_type":"markdown","source":["#### Why DenseNet-BC(L=100, k=12) ?\n","\n","Because, it is shown in DenseNet paper that this network configuration with data augmentation can have %error as minimal as 4.51 with just 0.8 million parameters which meets one of the Session4's criteria"]},{"metadata":{"id":"PE2R_CL8NaO1","colab_type":"text"},"cell_type":"markdown","source":["#### What is the best test score attained by your model?\n","My implementation of DenseNet-BC(L=100,k=12) as attained **94.31%** as test accuracy."]},{"metadata":{"colab_type":"text","id":"toOzuzCH2P82"},"cell_type":"markdown","source":["#### Network parameters as described in paper"]},{"metadata":{"colab_type":"code","id":"4Gu7cqp42Gvj","colab":{}},"cell_type":"code","source":["#number of layer in network\n","Layers = 100   \n","\n","#number of dense blocks\n","Dense_Blocks = 3   \n","\n","#growth rate\n","k = 12  \n","\n","#initial Conv feature output size, ie output channels of First Conv(3x3) layer in DenseNet\n","initial_out_features = k *2   \n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"ejeyOMG9MlN7"},"cell_type":"markdown","source":["\n","While using **DenseNet** with **BottleNeck(B)** layers, number of Conv(3x3) layers (**C**)\n","reduces by **1/2**, because with B layer, every composite layer(consisits of \n","batch norm, relu activation and conv operations in sequence) will now have\n","**C/2Conv(1x1)** layers and **C/2 Conv(3x3)** layers. \n","\n","For example if there were 32 Conv(3x3) layers in DenseNet network then DenseNet-B \n","will have 16 Conv(3x3) and 16 Conv(1x1) layers. \n","\n","So that for DenseNet-BC with depth L = 100,\n","- each dense block will have 16 Conv(3x3) and 16 Conv(1x1) layers\n","- and 3 Conv(1x1) layers from Transition layers\n","- and 1 Conv(3x3) layer from initial Conv(3x3) layers\n","\n","With 3 dense blocks\n","\n"," L = 16 \\* 3 + 16 * 3 + 3 + 1 = 100\n"," \n","Furthermore **VarianceScaling** is used as a kernel initalizer for DenseNet in paper, so the same initalizer is used for all Convolution layers\n"]},{"metadata":{"colab_type":"code","id":"EsMx54WJMltt","colab":{}},"cell_type":"code","source":["#composite layers per dense block\n","Composite_layer_per_dense_block = 16   \n","\n","\n","#number of filters in Conv(3x3) block\n","filters = k  \n","\n","#number of epochs\n","epochs = 250\n","\n","#number of classes\n","classes = 10\n","\n","#compression amount\n","compression_value = 0.5  \n","\n","#weight decay rate\n","weight_decay = 1e-4   \n","\n","#droput rate\n","dropout_value = 0.2   \n","\n","#momentum\n","momentum = 0.9   "],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"6-6LfvICNZG3"},"cell_type":"markdown","source":["#### Helper functions to build DenseNet"]},{"metadata":{"colab_type":"code","id":"XRYgGnDT_dSk","colab":{}},"cell_type":"code","source":["def get_composite_layer(Input_layer,\n","                        dropout_rate=dropout_value,\n","                        weight_decay=weight_decay,\n","                        growth_rate=k,\n","                        name_index=None\n","                       ):\n","  \"\"\"\n","  Function to create a composite block which invloves following ops\n","        Sub Block1:\n","        -----------\n","        BatchNormalization\n","        ReluActivation\n","        Conv2D(1x1)\n","        \n","        Sub Block2:\n","        -----------\n","        BatchNormalization\n","        ReluActivation\n","        Conv2D(3x3)\n","        Dropout\n","        \n","    Here Sub Block1 is called BottleNeck layer, it can significanlty reduce the number\n","    of input feature channels and there by increases the computational efficiency as refered in \n","    DenseNet paper. \n","    \n","    Sub Block2 is a normal Convolution(3x3) with dropout\n","    \n","    Input_layer: output of pervious layer\n","    \n","    num_filters: filter size for Conv() op\n","    \n","    dropout_rate: drop out\n","    \n","    weight_decay: decay value for regularising beta and gamma of batch normalization layer\n","    \n","    growth_rate: k,rate at which filters increases in sqeuence of composite layers\n","    \n","    name_index: index for layer naming\n","    \n","  \"\"\"\n","  # BottleNeck(B) composite ops\n","  channel_size = int(growth_rate * 4) #because in dense paper it is mentioned channel size\n","  # of B layer must be 4*k\n","  \n","  BN_B = BatchNormalization(beta_regularizer=l2(weight_decay), gamma_regularizer=l2(weight_decay))(Input_layer)\n","    \n","  ACTV_B = Activation('relu')(BN_B)\n","  \n","  CONV_B = Conv2D(filters=channel_size, \n","              kernel_size=(1,1), \n","              padding='same', \n","              use_bias=False,\n","              kernel_initializer='VarianceScaling',\n","             )(ACTV_B)\n","  \n","  \n","  # 3x3 Convolution composite ops\n","  BN = BatchNormalization(beta_regularizer=l2(weight_decay), gamma_regularizer=l2(weight_decay))(CONV_B)\n","    \n","  ACTV = Activation('relu')(BN)\n","  \n","  CONV = Conv2D(filters=filters, \n","              kernel_size=(3,3), \n","              strides=1, \n","              padding='same', \n","              use_bias=False,\n","              kernel_initializer='VarianceScaling',\n","             )(ACTV)\n","  DL = Dropout(dropout_rate)(CONV)\n","  \n","  return DL"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"mK_3taOwQQin","colab":{}},"cell_type":"code","source":["def get_transition_layer(Input_layer,\n","                    num_filters,\n","                    weight_decay=weight_decay,\n","                    compression=compression_value,\n","                    name_index=None):\n","  \"\"\"\n","  Function to get a transition layer which consists of \n","      \n","      BatchNormalization\n","      ReluActivation\n","      Conv2D(1x1)\n","      AveragePooling(2x2)\n","        \n","  Input_layer: output of pervious layer\n","  \n","  num_filters: filter size for Conv() op\n","  \n","  dropout_rate: drop out\n","  \n","  weight_decay: decay value for regularising beta and gamma of batch normalization layer\n","  \n","  compression: value between 0 and 1, for reducing the depth of channels from previous layers\n","  \n","  name_index: index for layer naming\n","    \n","  \"\"\"\n","  BN = BatchNormalization(beta_regularizer=l2(weight_decay), gamma_regularizer=l2(weight_decay))(Input_layer)\n","  \n","  ACTV = Activation('relu')(BN)\n","  compressed_channels = int(num_filters*compression)\n","  CONV = Conv2D(filters=compressed_channels, \n","              kernel_size=(1,1), \n","              strides=1, \n","              padding='same', \n","              kernel_initializer='VarianceScaling',\n","              use_bias=False,\n","             )(ACTV)\n","  AVGP = AveragePooling2D((2,2))(CONV)\n","  \n","  return AVGP"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"dScil2DET0JA","colab":{}},"cell_type":"code","source":["def create_dense_block(Input_layer,\n","               num_layers = Composite_layer_per_dense_block,\n","               num_filters =filters,\n","               grow_rate=k,\n","               dropout_rate=dropout_value,\n","               weight_decay=weight_decay,\n","               ):\n","  \"\"\"\n","  Function to create multiple Conv layer in Dense layer in a loop \n","  \n","  Input_layer: output of pervious layer\n","  \n","  num_layers: total number of Conv layers per dense block\n","  \n","  num_filters: filter size for Conv() op\n","  \n","  growth_rate: k, the rate at which channel depth increases during a sequence of Conv operations\n","  \n","  dropout_rate: dropout\n","  \n","  weight_decay: decay value for regularising beta and gamma of batch normalization layer\n","\n","  \"\"\"\n","  layers = [Input_layer]\n","    \n","  #creating multiple composite layers\n","  for i in range(num_layers):\n","      composite_layer = get_composite_layer(Input_layer,\n","                                            dropout_rate=dropout_rate,\n","                                            weight_decay=weight_decay,\n","                                           name_index = i)\n","      #stores all composite layers\n","      layers.append(composite_layer)\n","\n","      #concatenate layers along depth with k as growth rate\n","      Input_layer = Concatenate(axis=-1)(layers)\n","      num_filters = num_filters + grow_rate\n","        \n","  return Input_layer, num_filters"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"NL-utcwvNfaS"},"cell_type":"markdown","source":["#### DenseNet model"]},{"metadata":{"colab_type":"code","id":"048cVLYdT03C","colab":{}},"cell_type":"code","source":["def create_densenet_network(base_model=False,\n","                            num_dense_blocks=Dense_Blocks,\n","                            layer_per_dense_block=Composite_layer_per_dense_block,\n","                            num_filters=filters,\n","                            growth_rate=k,\n","                            num_classes=classes,\n","                            weight_decay=weight_decay,\n","                            dropout_rate=dropout_value,\n","                            compression=compression_value,\n","                            low_res_dim=20\n","                           ):\n","  \"\"\"\n","  Function to create a DenseNet-BC network(L=100, k=12) ie,with total 100 layers and growth rate 12\n","  \n","  base_model: bool, to train model on low resolution image\n","  \n","  num_dense_blocks: number of dense blocks in network \n","  \n","  layer_per_dense_block: number of convolution layers per dense block \n","  \n","  num_filters: output filter/channel size for Conv(3x3) layers in DenseBlocks\n","  \n","  grow_rate: k\n","  \n","  num_classes: number of classes\n","  \n","  weight_decay: decay rate for batch normalization regularisation\n","  \n","  dropout_rate: dropout rate\n","  \n","  compression: channel compression value\n","  \n","  low_res_dim: reduced resolution of input for base model training to initialise weights\n","  \"\"\"\n","  \n","  #algining input dimensions based on mode of training\n","  if not base_model:    \n","    images = Input(shape=(32,32,3))#x_train_a.shape[1:]\n","  else:\n","    images = Input(shape=(20,20,3))\n","        \n","  #First pure Conv(3x3) layer with 2*k output channels as per paper\n","  Conv_1 = Conv2D(int(k*2),\n","                   (3,3),\n","                   padding='same', \n","                   use_bias=False,\n","                   kernel_regularizer=l2(weight_decay),\n","                   kernel_initializer='VarianceScaling',\n","                   name=\"First_Conv2D\"\n","                 )(images)\n","\n","  #Creating first num_dense_blocks-1 dense blocks along with transition blocks\n","  for i in range(0,Dense_Blocks-1):\n","    # to handle connection between first Conv layer and other composite layers\n","    if i == 0:\n","      layers, num_filters = create_dense_block(Input_layer=Conv_1,num_filters=num_filters)\n","      layers = get_transition_layer(Input_layer=layers,num_filters=num_filters )\n","      \n","    else:\n","      layers, num_filters = create_dense_block(Input_layer=layers, num_filters=num_filters)\n","      layers = get_transition_layer(Input_layer=layers,num_filters=num_filters)\n","\n","  #last dense block \n","  layers, num_filters = create_dense_block(Input_layer=layers,num_filters=num_filters)\n","\n","  #output layer\n","  bn = BatchNormalization(gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(layers)\n","  \n","  actv = Activation('relu')(bn)\n","  \n","  pool_layer = GlobalAveragePooling2D()(actv)\n","  \n","  fc_layer = Dense(classes,\n","                   activation='softmax',\n","                   kernel_regularizer=l2(weight_decay),\n","                   bias_regularizer=l2(weight_decay))(pool_layer)\n","  \n","  model = Model(inputs=images, outputs=[fc_layer])\n","  return model\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"KUBUZwW7Nvv0"},"cell_type":"markdown","source":["### Data preprocessing and augmentation"]},{"metadata":{"colab_type":"text","id":"Nk6L2UaROTQ_"},"cell_type":"markdown","source":["Augmented data size is **twice** the training data size because many of parctical implementations of DenseNet used this to attain an accuracy close to the one stated in DenseNet paper."]},{"metadata":{"colab_type":"code","id":"sGGX3ZZUN2Uy","outputId":"018ebe48-01d6-49d4-882a-68155c692b60","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1541410484443,"user_tz":480,"elapsed":79418,"user":{"displayName":"satish jasti","photoUrl":"https://lh5.googleusercontent.com/-2QUvh0bgop8/AAAAAAAAAAI/AAAAAAAACeQ/iMC5VLpeeGw/s64/photo.jpg","userId":"08942596929441220059"}}},"cell_type":"code","source":["#Augmenting original 32x32,3 data\n","class CIFAR:\n"," \n","    def __init__(self):\n","        (self.x_train, self.y_train), (self.x_test, self.y_test) = self.load_cifar()\n","        assert self.x_train.shape[1:] == (32,32,3), 'x_train is not of shape (32,32,3)'\n","        assert self.x_test.shape[1:] == (32,32,3), 'x_train is not of shape (32,32,3)'\n","        \n","        assert self.y_train.shape[1] == 1, 'y_train is not of shape (None, 1)'\n","        assert self.y_test.shape[1] == 1, 'y_test is not of shape (None,1)'\n","        \n","        self.train_size = self.x_train.shape[0]\n","        assert self.train_size == 50000, 'training samples not equal to 50k'\n","        self.test_size = self.x_test.shape[0]\n","        assert self.test_size == 10000, 'test samples not equal to 10k'\n","        \n","    def load_cifar(self):\n","        \"\"\" Load the CIFAR10 dataset, normalise it, and return it as a tuple \"\"\"\n","        (self.x_train, self.y_train), (self.x_test, self.y_test) = cifar10.load_data()\n","        self.x_train = self.x_train.astype('float32')\n","        self.x_test = self.x_test.astype('float32')\n","        X = np.vstack((self.x_train, self.x_test))\n","        assert X.shape == (60000, 32, 32, 3), 'Unable to combine test and train data for preprocessing'\n","        \n","        #normalize data\n","        for i in range(3):\n","            mean = np.mean(X[:, :, :, i])\n","            std = np.std(X[:, :, :, i])\n","            self.x_train[:, :, :, i] = (self.x_train[:, :, :, i] - mean) / std\n","            self.x_test[:, :, :, i] = (self.x_test[:, :, :, i] - mean) / std\n","        return (self.x_train, self.y_train), (self.x_test, self.y_test)\n","      \n","\n","    def data_augmentation(self, augment_size=100000): \n","        image_generator = ImageDataGenerator(\n","                                            rotation_range=30,\n","                                            height_shift_range=0.1, \n","                                            width_shift_range=0.1,\n","                                            horizontal_flip=True,                                    \n","                                            )\n","        \n","        # randomly rotate,flip and shift images\n","        image_generator.fit(self.x_train, augment=True)\n","        \n","        # fetch transformed images\n","        #generate random indices between 1 and 50k\n","        randidx = np.random.randint(self.train_size, size=augment_size)\n","        \n","        #copy images with all indicies in randinx to x_augmented\n","        x_augmented = self.x_train[randidx].copy()\n","        assert x_augmented.shape[0] == augment_size , 'Unable to augment data with {} smaples'.format(augment_size)\n","        \n","        y_augmented = self.y_train[randidx].copy()\n","        assert y_augmented.shape[0] == augment_size , 'Unable to augment data with {} smaples'.format(augment_size)\n","        \n","        \n","        x_augmented = image_generator.flow(x_augmented, np.zeros(augment_size),\n","                                    batch_size=augment_size, shuffle=False).next()[0]\n","        \n","        # append augmented data to trainset\n","        self.x_train = np.concatenate((self.x_train, x_augmented))\n","        self.y_train = np.concatenate((self.y_train, y_augmented))\n","        self.train_size = self.x_train.shape[0]\n","        self.test_size = self.x_test.shape[0]\n","        \n","\n","#get data \n","data_loader = CIFAR()\n","data_loader.data_augmentation()\n","x_train_a, y_train_a, x_test, y_test = data_loader.x_train, data_loader.y_train, data_loader.x_test, data_loader.y_test\n","\n","\n","#converting y_values into one hot encoding\n","y_train_a_h = np_utils.to_categorical(y_train_a, 10)\n","y_test_h = np_utils.to_categorical(y_test, 10)\n","\n","# #dimensions of augmented images\n","x_train_a.shape, y_train_a_h.shape, x_test.shape, y_test_h.shape"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 6s 0us/step\n","170508288/170498071 [==============================] - 6s 0us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["((150000, 32, 32, 3), (150000, 10), (10000, 32, 32, 3), (10000, 10))"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"colab_type":"text","id":"TG1EpxjsRW7z"},"cell_type":"markdown","source":["### Train DenseNet-BC"]},{"metadata":{"id":"htzGF-LnNzFz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":20604},"outputId":"534755a3-1721-4772-b0d9-15de052f7a8f","executionInfo":{"status":"ok","timestamp":1541410501587,"user_tz":480,"elapsed":17109,"user":{"displayName":"satish jasti","photoUrl":"https://lh5.googleusercontent.com/-2QUvh0bgop8/AAAAAAAAAAI/AAAAAAAACeQ/iMC5VLpeeGw/s64/photo.jpg","userId":"08942596929441220059"}}},"cell_type":"code","source":["#create model\n","model = create_densenet_network()\n","model.summary()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","First_Conv2D (Conv2D)           (None, 32, 32, 24)   648         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 32, 32, 24)   96          First_Conv2D[0][0]               \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 32, 32, 24)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 32, 32, 48)   1152        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 32, 32, 48)   192         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 32, 32, 48)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 32, 32, 12)   5184        activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 32, 32, 12)   0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 32, 32, 36)   0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 32, 32, 36)   144         concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 32, 32, 36)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 32, 32, 48)   1728        activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 32, 32, 48)   192         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 32, 32, 12)   5184        activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 32, 32, 12)   0           conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 32, 32, 48)   0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 32, 32, 48)   192         concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 32, 32, 48)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 32, 32, 48)   2304        activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 32, 32, 48)   192         conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 32, 32, 48)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 32, 32, 12)   5184        activation_6[0][0]               \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 32, 32, 12)   0           conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 32, 32, 60)   0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 32, 32, 60)   240         concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 32, 32, 60)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 32, 32, 48)   2880        activation_7[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 32, 32, 48)   192         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 32, 32, 48)   0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 32, 32, 12)   5184        activation_8[0][0]               \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 32, 32, 12)   0           conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 32, 32, 72)   0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 32, 32, 72)   288         concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 32, 32, 72)   0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 32, 32, 48)   3456        activation_9[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 32, 32, 48)   192         conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 32, 32, 48)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 32, 32, 12)   5184        activation_10[0][0]              \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 32, 32, 12)   0           conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, 32, 32, 84)   0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 32, 32, 84)   336         concatenate_5[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 32, 32, 84)   0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 32, 32, 48)   4032        activation_11[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 32, 32, 48)   192         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 32, 32, 48)   0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 32, 32, 12)   5184        activation_12[0][0]              \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 32, 32, 12)   0           conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_6 (Concatenate)     (None, 32, 32, 96)   0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 32, 32, 96)   384         concatenate_6[0][0]              \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 32, 32, 96)   0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 32, 32, 48)   4608        activation_13[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 32, 32, 48)   192         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 32, 32, 48)   0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 32, 32, 12)   5184        activation_14[0][0]              \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 32, 32, 12)   0           conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_7 (Concatenate)     (None, 32, 32, 108)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 32, 32, 108)  432         concatenate_7[0][0]              \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 32, 32, 108)  0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 32, 32, 48)   5184        activation_15[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 32, 32, 48)   192         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 32, 32, 48)   0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 32, 32, 12)   5184        activation_16[0][0]              \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 32, 32, 12)   0           conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_8 (Concatenate)     (None, 32, 32, 120)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","                                                                 dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 32, 32, 120)  480         concatenate_8[0][0]              \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 32, 32, 120)  0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 32, 32, 48)   5760        activation_17[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 32, 32, 48)   192         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 32, 32, 48)   0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 32, 32, 12)   5184        activation_18[0][0]              \n","__________________________________________________________________________________________________\n","dropout_9 (Dropout)             (None, 32, 32, 12)   0           conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_9 (Concatenate)     (None, 32, 32, 132)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","                                                                 dropout_8[0][0]                  \n","                                                                 dropout_9[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 32, 32, 132)  528         concatenate_9[0][0]              \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 32, 32, 132)  0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 32, 32, 48)   6336        activation_19[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 32, 32, 48)   192         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 32, 32, 48)   0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 32, 32, 12)   5184        activation_20[0][0]              \n","__________________________________________________________________________________________________\n","dropout_10 (Dropout)            (None, 32, 32, 12)   0           conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_10 (Concatenate)    (None, 32, 32, 144)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","                                                                 dropout_8[0][0]                  \n","                                                                 dropout_9[0][0]                  \n","                                                                 dropout_10[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 32, 32, 144)  576         concatenate_10[0][0]             \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 32, 32, 144)  0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 32, 32, 48)   6912        activation_21[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 32, 32, 48)   192         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 32, 32, 48)   0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 32, 32, 12)   5184        activation_22[0][0]              \n","__________________________________________________________________________________________________\n","dropout_11 (Dropout)            (None, 32, 32, 12)   0           conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_11 (Concatenate)    (None, 32, 32, 156)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","                                                                 dropout_8[0][0]                  \n","                                                                 dropout_9[0][0]                  \n","                                                                 dropout_10[0][0]                 \n","                                                                 dropout_11[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 32, 32, 156)  624         concatenate_11[0][0]             \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 32, 32, 156)  0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 32, 32, 48)   7488        activation_23[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 32, 32, 48)   192         conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 32, 32, 48)   0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 32, 32, 12)   5184        activation_24[0][0]              \n","__________________________________________________________________________________________________\n","dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_24[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_12 (Concatenate)    (None, 32, 32, 168)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","                                                                 dropout_8[0][0]                  \n","                                                                 dropout_9[0][0]                  \n","                                                                 dropout_10[0][0]                 \n","                                                                 dropout_11[0][0]                 \n","                                                                 dropout_12[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 32, 32, 168)  672         concatenate_12[0][0]             \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 32, 32, 168)  0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 32, 32, 48)   8064        activation_25[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 32, 32, 48)   192         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 32, 32, 48)   0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 32, 32, 12)   5184        activation_26[0][0]              \n","__________________________________________________________________________________________________\n","dropout_13 (Dropout)            (None, 32, 32, 12)   0           conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_13 (Concatenate)    (None, 32, 32, 180)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","                                                                 dropout_8[0][0]                  \n","                                                                 dropout_9[0][0]                  \n","                                                                 dropout_10[0][0]                 \n","                                                                 dropout_11[0][0]                 \n","                                                                 dropout_12[0][0]                 \n","                                                                 dropout_13[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 32, 32, 180)  720         concatenate_13[0][0]             \n","__________________________________________________________________________________________________\n","activation_27 (Activation)      (None, 32, 32, 180)  0           batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 32, 32, 48)   8640        activation_27[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 32, 32, 48)   192         conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","activation_28 (Activation)      (None, 32, 32, 48)   0           batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 32, 32, 12)   5184        activation_28[0][0]              \n","__________________________________________________________________________________________________\n","dropout_14 (Dropout)            (None, 32, 32, 12)   0           conv2d_28[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_14 (Concatenate)    (None, 32, 32, 192)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","                                                                 dropout_8[0][0]                  \n","                                                                 dropout_9[0][0]                  \n","                                                                 dropout_10[0][0]                 \n","                                                                 dropout_11[0][0]                 \n","                                                                 dropout_12[0][0]                 \n","                                                                 dropout_13[0][0]                 \n","                                                                 dropout_14[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_29 (BatchNo (None, 32, 32, 192)  768         concatenate_14[0][0]             \n","__________________________________________________________________________________________________\n","activation_29 (Activation)      (None, 32, 32, 192)  0           batch_normalization_29[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 32, 32, 48)   9216        activation_29[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_30 (BatchNo (None, 32, 32, 48)   192         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","activation_30 (Activation)      (None, 32, 32, 48)   0           batch_normalization_30[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 32, 32, 12)   5184        activation_30[0][0]              \n","__________________________________________________________________________________________________\n","dropout_15 (Dropout)            (None, 32, 32, 12)   0           conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_15 (Concatenate)    (None, 32, 32, 204)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","                                                                 dropout_8[0][0]                  \n","                                                                 dropout_9[0][0]                  \n","                                                                 dropout_10[0][0]                 \n","                                                                 dropout_11[0][0]                 \n","                                                                 dropout_12[0][0]                 \n","                                                                 dropout_13[0][0]                 \n","                                                                 dropout_14[0][0]                 \n","                                                                 dropout_15[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_31 (BatchNo (None, 32, 32, 204)  816         concatenate_15[0][0]             \n","__________________________________________________________________________________________________\n","activation_31 (Activation)      (None, 32, 32, 204)  0           batch_normalization_31[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 32, 32, 48)   9792        activation_31[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_32 (BatchNo (None, 32, 32, 48)   192         conv2d_31[0][0]                  \n","__________________________________________________________________________________________________\n","activation_32 (Activation)      (None, 32, 32, 48)   0           batch_normalization_32[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 32, 32, 12)   5184        activation_32[0][0]              \n","__________________________________________________________________________________________________\n","dropout_16 (Dropout)            (None, 32, 32, 12)   0           conv2d_32[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_16 (Concatenate)    (None, 32, 32, 216)  0           First_Conv2D[0][0]               \n","                                                                 dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","                                                                 dropout_3[0][0]                  \n","                                                                 dropout_4[0][0]                  \n","                                                                 dropout_5[0][0]                  \n","                                                                 dropout_6[0][0]                  \n","                                                                 dropout_7[0][0]                  \n","                                                                 dropout_8[0][0]                  \n","                                                                 dropout_9[0][0]                  \n","                                                                 dropout_10[0][0]                 \n","                                                                 dropout_11[0][0]                 \n","                                                                 dropout_12[0][0]                 \n","                                                                 dropout_13[0][0]                 \n","                                                                 dropout_14[0][0]                 \n","                                                                 dropout_15[0][0]                 \n","                                                                 dropout_16[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_33 (BatchNo (None, 32, 32, 216)  864         concatenate_16[0][0]             \n","__________________________________________________________________________________________________\n","activation_33 (Activation)      (None, 32, 32, 216)  0           batch_normalization_33[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 32, 32, 102)  22032       activation_33[0][0]              \n","__________________________________________________________________________________________________\n","average_pooling2d_1 (AveragePoo (None, 16, 16, 102)  0           conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_34 (BatchNo (None, 16, 16, 102)  408         average_pooling2d_1[0][0]        \n","__________________________________________________________________________________________________\n","activation_34 (Activation)      (None, 16, 16, 102)  0           batch_normalization_34[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 16, 16, 48)   4896        activation_34[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_35 (BatchNo (None, 16, 16, 48)   192         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","activation_35 (Activation)      (None, 16, 16, 48)   0           batch_normalization_35[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 16, 16, 12)   5184        activation_35[0][0]              \n","__________________________________________________________________________________________________\n","dropout_17 (Dropout)            (None, 16, 16, 12)   0           conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_17 (Concatenate)    (None, 16, 16, 114)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_36 (BatchNo (None, 16, 16, 114)  456         concatenate_17[0][0]             \n","__________________________________________________________________________________________________\n","activation_36 (Activation)      (None, 16, 16, 114)  0           batch_normalization_36[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 16, 16, 48)   5472        activation_36[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_37 (BatchNo (None, 16, 16, 48)   192         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","activation_37 (Activation)      (None, 16, 16, 48)   0           batch_normalization_37[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 16, 16, 12)   5184        activation_37[0][0]              \n","__________________________________________________________________________________________________\n","dropout_18 (Dropout)            (None, 16, 16, 12)   0           conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_18 (Concatenate)    (None, 16, 16, 126)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_38 (BatchNo (None, 16, 16, 126)  504         concatenate_18[0][0]             \n","__________________________________________________________________________________________________\n","activation_38 (Activation)      (None, 16, 16, 126)  0           batch_normalization_38[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 16, 16, 48)   6048        activation_38[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_39 (BatchNo (None, 16, 16, 48)   192         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_39 (Activation)      (None, 16, 16, 48)   0           batch_normalization_39[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 16, 16, 12)   5184        activation_39[0][0]              \n","__________________________________________________________________________________________________\n","dropout_19 (Dropout)            (None, 16, 16, 12)   0           conv2d_39[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_19 (Concatenate)    (None, 16, 16, 138)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_40 (BatchNo (None, 16, 16, 138)  552         concatenate_19[0][0]             \n","__________________________________________________________________________________________________\n","activation_40 (Activation)      (None, 16, 16, 138)  0           batch_normalization_40[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 16, 16, 48)   6624        activation_40[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_41 (BatchNo (None, 16, 16, 48)   192         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","activation_41 (Activation)      (None, 16, 16, 48)   0           batch_normalization_41[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 16, 16, 12)   5184        activation_41[0][0]              \n","__________________________________________________________________________________________________\n","dropout_20 (Dropout)            (None, 16, 16, 12)   0           conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_20 (Concatenate)    (None, 16, 16, 150)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_42 (BatchNo (None, 16, 16, 150)  600         concatenate_20[0][0]             \n","__________________________________________________________________________________________________\n","activation_42 (Activation)      (None, 16, 16, 150)  0           batch_normalization_42[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 16, 16, 48)   7200        activation_42[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_43 (BatchNo (None, 16, 16, 48)   192         conv2d_42[0][0]                  \n","__________________________________________________________________________________________________\n","activation_43 (Activation)      (None, 16, 16, 48)   0           batch_normalization_43[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 16, 16, 12)   5184        activation_43[0][0]              \n","__________________________________________________________________________________________________\n","dropout_21 (Dropout)            (None, 16, 16, 12)   0           conv2d_43[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_21 (Concatenate)    (None, 16, 16, 162)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_44 (BatchNo (None, 16, 16, 162)  648         concatenate_21[0][0]             \n","__________________________________________________________________________________________________\n","activation_44 (Activation)      (None, 16, 16, 162)  0           batch_normalization_44[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 16, 16, 48)   7776        activation_44[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_45 (BatchNo (None, 16, 16, 48)   192         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","activation_45 (Activation)      (None, 16, 16, 48)   0           batch_normalization_45[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 16, 16, 12)   5184        activation_45[0][0]              \n","__________________________________________________________________________________________________\n","dropout_22 (Dropout)            (None, 16, 16, 12)   0           conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_22 (Concatenate)    (None, 16, 16, 174)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_46 (BatchNo (None, 16, 16, 174)  696         concatenate_22[0][0]             \n","__________________________________________________________________________________________________\n","activation_46 (Activation)      (None, 16, 16, 174)  0           batch_normalization_46[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 16, 16, 48)   8352        activation_46[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_47 (BatchNo (None, 16, 16, 48)   192         conv2d_46[0][0]                  \n","__________________________________________________________________________________________________\n","activation_47 (Activation)      (None, 16, 16, 48)   0           batch_normalization_47[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 16, 16, 12)   5184        activation_47[0][0]              \n","__________________________________________________________________________________________________\n","dropout_23 (Dropout)            (None, 16, 16, 12)   0           conv2d_47[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_23 (Concatenate)    (None, 16, 16, 186)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_48 (BatchNo (None, 16, 16, 186)  744         concatenate_23[0][0]             \n","__________________________________________________________________________________________________\n","activation_48 (Activation)      (None, 16, 16, 186)  0           batch_normalization_48[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 16, 16, 48)   8928        activation_48[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_49 (BatchNo (None, 16, 16, 48)   192         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","activation_49 (Activation)      (None, 16, 16, 48)   0           batch_normalization_49[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 16, 16, 12)   5184        activation_49[0][0]              \n","__________________________________________________________________________________________________\n","dropout_24 (Dropout)            (None, 16, 16, 12)   0           conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_24 (Concatenate)    (None, 16, 16, 198)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","                                                                 dropout_24[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_50 (BatchNo (None, 16, 16, 198)  792         concatenate_24[0][0]             \n","__________________________________________________________________________________________________\n","activation_50 (Activation)      (None, 16, 16, 198)  0           batch_normalization_50[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 16, 16, 48)   9504        activation_50[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_51 (BatchNo (None, 16, 16, 48)   192         conv2d_50[0][0]                  \n","__________________________________________________________________________________________________\n","activation_51 (Activation)      (None, 16, 16, 48)   0           batch_normalization_51[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 16, 16, 12)   5184        activation_51[0][0]              \n","__________________________________________________________________________________________________\n","dropout_25 (Dropout)            (None, 16, 16, 12)   0           conv2d_51[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_25 (Concatenate)    (None, 16, 16, 210)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","                                                                 dropout_24[0][0]                 \n","                                                                 dropout_25[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_52 (BatchNo (None, 16, 16, 210)  840         concatenate_25[0][0]             \n","__________________________________________________________________________________________________\n","activation_52 (Activation)      (None, 16, 16, 210)  0           batch_normalization_52[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 16, 16, 48)   10080       activation_52[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_53 (BatchNo (None, 16, 16, 48)   192         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","activation_53 (Activation)      (None, 16, 16, 48)   0           batch_normalization_53[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 16, 16, 12)   5184        activation_53[0][0]              \n","__________________________________________________________________________________________________\n","dropout_26 (Dropout)            (None, 16, 16, 12)   0           conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_26 (Concatenate)    (None, 16, 16, 222)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","                                                                 dropout_24[0][0]                 \n","                                                                 dropout_25[0][0]                 \n","                                                                 dropout_26[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_54 (BatchNo (None, 16, 16, 222)  888         concatenate_26[0][0]             \n","__________________________________________________________________________________________________\n","activation_54 (Activation)      (None, 16, 16, 222)  0           batch_normalization_54[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 16, 16, 48)   10656       activation_54[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_55 (BatchNo (None, 16, 16, 48)   192         conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","activation_55 (Activation)      (None, 16, 16, 48)   0           batch_normalization_55[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 16, 16, 12)   5184        activation_55[0][0]              \n","__________________________________________________________________________________________________\n","dropout_27 (Dropout)            (None, 16, 16, 12)   0           conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_27 (Concatenate)    (None, 16, 16, 234)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","                                                                 dropout_24[0][0]                 \n","                                                                 dropout_25[0][0]                 \n","                                                                 dropout_26[0][0]                 \n","                                                                 dropout_27[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_56 (BatchNo (None, 16, 16, 234)  936         concatenate_27[0][0]             \n","__________________________________________________________________________________________________\n","activation_56 (Activation)      (None, 16, 16, 234)  0           batch_normalization_56[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 16, 16, 48)   11232       activation_56[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_57 (BatchNo (None, 16, 16, 48)   192         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","activation_57 (Activation)      (None, 16, 16, 48)   0           batch_normalization_57[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_57 (Conv2D)              (None, 16, 16, 12)   5184        activation_57[0][0]              \n","__________________________________________________________________________________________________\n","dropout_28 (Dropout)            (None, 16, 16, 12)   0           conv2d_57[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_28 (Concatenate)    (None, 16, 16, 246)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","                                                                 dropout_24[0][0]                 \n","                                                                 dropout_25[0][0]                 \n","                                                                 dropout_26[0][0]                 \n","                                                                 dropout_27[0][0]                 \n","                                                                 dropout_28[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_58 (BatchNo (None, 16, 16, 246)  984         concatenate_28[0][0]             \n","__________________________________________________________________________________________________\n","activation_58 (Activation)      (None, 16, 16, 246)  0           batch_normalization_58[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_58 (Conv2D)              (None, 16, 16, 48)   11808       activation_58[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_59 (BatchNo (None, 16, 16, 48)   192         conv2d_58[0][0]                  \n","__________________________________________________________________________________________________\n","activation_59 (Activation)      (None, 16, 16, 48)   0           batch_normalization_59[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_59 (Conv2D)              (None, 16, 16, 12)   5184        activation_59[0][0]              \n","__________________________________________________________________________________________________\n","dropout_29 (Dropout)            (None, 16, 16, 12)   0           conv2d_59[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_29 (Concatenate)    (None, 16, 16, 258)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","                                                                 dropout_24[0][0]                 \n","                                                                 dropout_25[0][0]                 \n","                                                                 dropout_26[0][0]                 \n","                                                                 dropout_27[0][0]                 \n","                                                                 dropout_28[0][0]                 \n","                                                                 dropout_29[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_60 (BatchNo (None, 16, 16, 258)  1032        concatenate_29[0][0]             \n","__________________________________________________________________________________________________\n","activation_60 (Activation)      (None, 16, 16, 258)  0           batch_normalization_60[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_60 (Conv2D)              (None, 16, 16, 48)   12384       activation_60[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_61 (BatchNo (None, 16, 16, 48)   192         conv2d_60[0][0]                  \n","__________________________________________________________________________________________________\n","activation_61 (Activation)      (None, 16, 16, 48)   0           batch_normalization_61[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_61 (Conv2D)              (None, 16, 16, 12)   5184        activation_61[0][0]              \n","__________________________________________________________________________________________________\n","dropout_30 (Dropout)            (None, 16, 16, 12)   0           conv2d_61[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_30 (Concatenate)    (None, 16, 16, 270)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","                                                                 dropout_24[0][0]                 \n","                                                                 dropout_25[0][0]                 \n","                                                                 dropout_26[0][0]                 \n","                                                                 dropout_27[0][0]                 \n","                                                                 dropout_28[0][0]                 \n","                                                                 dropout_29[0][0]                 \n","                                                                 dropout_30[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_62 (BatchNo (None, 16, 16, 270)  1080        concatenate_30[0][0]             \n","__________________________________________________________________________________________________\n","activation_62 (Activation)      (None, 16, 16, 270)  0           batch_normalization_62[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_62 (Conv2D)              (None, 16, 16, 48)   12960       activation_62[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_63 (BatchNo (None, 16, 16, 48)   192         conv2d_62[0][0]                  \n","__________________________________________________________________________________________________\n","activation_63 (Activation)      (None, 16, 16, 48)   0           batch_normalization_63[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_63 (Conv2D)              (None, 16, 16, 12)   5184        activation_63[0][0]              \n","__________________________________________________________________________________________________\n","dropout_31 (Dropout)            (None, 16, 16, 12)   0           conv2d_63[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_31 (Concatenate)    (None, 16, 16, 282)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","                                                                 dropout_24[0][0]                 \n","                                                                 dropout_25[0][0]                 \n","                                                                 dropout_26[0][0]                 \n","                                                                 dropout_27[0][0]                 \n","                                                                 dropout_28[0][0]                 \n","                                                                 dropout_29[0][0]                 \n","                                                                 dropout_30[0][0]                 \n","                                                                 dropout_31[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_64 (BatchNo (None, 16, 16, 282)  1128        concatenate_31[0][0]             \n","__________________________________________________________________________________________________\n","activation_64 (Activation)      (None, 16, 16, 282)  0           batch_normalization_64[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_64 (Conv2D)              (None, 16, 16, 48)   13536       activation_64[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_65 (BatchNo (None, 16, 16, 48)   192         conv2d_64[0][0]                  \n","__________________________________________________________________________________________________\n","activation_65 (Activation)      (None, 16, 16, 48)   0           batch_normalization_65[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_65 (Conv2D)              (None, 16, 16, 12)   5184        activation_65[0][0]              \n","__________________________________________________________________________________________________\n","dropout_32 (Dropout)            (None, 16, 16, 12)   0           conv2d_65[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_32 (Concatenate)    (None, 16, 16, 294)  0           average_pooling2d_1[0][0]        \n","                                                                 dropout_17[0][0]                 \n","                                                                 dropout_18[0][0]                 \n","                                                                 dropout_19[0][0]                 \n","                                                                 dropout_20[0][0]                 \n","                                                                 dropout_21[0][0]                 \n","                                                                 dropout_22[0][0]                 \n","                                                                 dropout_23[0][0]                 \n","                                                                 dropout_24[0][0]                 \n","                                                                 dropout_25[0][0]                 \n","                                                                 dropout_26[0][0]                 \n","                                                                 dropout_27[0][0]                 \n","                                                                 dropout_28[0][0]                 \n","                                                                 dropout_29[0][0]                 \n","                                                                 dropout_30[0][0]                 \n","                                                                 dropout_31[0][0]                 \n","                                                                 dropout_32[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_66 (BatchNo (None, 16, 16, 294)  1176        concatenate_32[0][0]             \n","__________________________________________________________________________________________________\n","activation_66 (Activation)      (None, 16, 16, 294)  0           batch_normalization_66[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_66 (Conv2D)              (None, 16, 16, 198)  58212       activation_66[0][0]              \n","__________________________________________________________________________________________________\n","average_pooling2d_2 (AveragePoo (None, 8, 8, 198)    0           conv2d_66[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_67 (BatchNo (None, 8, 8, 198)    792         average_pooling2d_2[0][0]        \n","__________________________________________________________________________________________________\n","activation_67 (Activation)      (None, 8, 8, 198)    0           batch_normalization_67[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_67 (Conv2D)              (None, 8, 8, 48)     9504        activation_67[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_68 (BatchNo (None, 8, 8, 48)     192         conv2d_67[0][0]                  \n","__________________________________________________________________________________________________\n","activation_68 (Activation)      (None, 8, 8, 48)     0           batch_normalization_68[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_68 (Conv2D)              (None, 8, 8, 12)     5184        activation_68[0][0]              \n","__________________________________________________________________________________________________\n","dropout_33 (Dropout)            (None, 8, 8, 12)     0           conv2d_68[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_33 (Concatenate)    (None, 8, 8, 210)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_69 (BatchNo (None, 8, 8, 210)    840         concatenate_33[0][0]             \n","__________________________________________________________________________________________________\n","activation_69 (Activation)      (None, 8, 8, 210)    0           batch_normalization_69[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_69 (Conv2D)              (None, 8, 8, 48)     10080       activation_69[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_70 (BatchNo (None, 8, 8, 48)     192         conv2d_69[0][0]                  \n","__________________________________________________________________________________________________\n","activation_70 (Activation)      (None, 8, 8, 48)     0           batch_normalization_70[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_70 (Conv2D)              (None, 8, 8, 12)     5184        activation_70[0][0]              \n","__________________________________________________________________________________________________\n","dropout_34 (Dropout)            (None, 8, 8, 12)     0           conv2d_70[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_34 (Concatenate)    (None, 8, 8, 222)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_71 (BatchNo (None, 8, 8, 222)    888         concatenate_34[0][0]             \n","__________________________________________________________________________________________________\n","activation_71 (Activation)      (None, 8, 8, 222)    0           batch_normalization_71[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_71 (Conv2D)              (None, 8, 8, 48)     10656       activation_71[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_72 (BatchNo (None, 8, 8, 48)     192         conv2d_71[0][0]                  \n","__________________________________________________________________________________________________\n","activation_72 (Activation)      (None, 8, 8, 48)     0           batch_normalization_72[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_72 (Conv2D)              (None, 8, 8, 12)     5184        activation_72[0][0]              \n","__________________________________________________________________________________________________\n","dropout_35 (Dropout)            (None, 8, 8, 12)     0           conv2d_72[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_35 (Concatenate)    (None, 8, 8, 234)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_73 (BatchNo (None, 8, 8, 234)    936         concatenate_35[0][0]             \n","__________________________________________________________________________________________________\n","activation_73 (Activation)      (None, 8, 8, 234)    0           batch_normalization_73[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_73 (Conv2D)              (None, 8, 8, 48)     11232       activation_73[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_74 (BatchNo (None, 8, 8, 48)     192         conv2d_73[0][0]                  \n","__________________________________________________________________________________________________\n","activation_74 (Activation)      (None, 8, 8, 48)     0           batch_normalization_74[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_74 (Conv2D)              (None, 8, 8, 12)     5184        activation_74[0][0]              \n","__________________________________________________________________________________________________\n","dropout_36 (Dropout)            (None, 8, 8, 12)     0           conv2d_74[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_36 (Concatenate)    (None, 8, 8, 246)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_75 (BatchNo (None, 8, 8, 246)    984         concatenate_36[0][0]             \n","__________________________________________________________________________________________________\n","activation_75 (Activation)      (None, 8, 8, 246)    0           batch_normalization_75[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_75 (Conv2D)              (None, 8, 8, 48)     11808       activation_75[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_76 (BatchNo (None, 8, 8, 48)     192         conv2d_75[0][0]                  \n","__________________________________________________________________________________________________\n","activation_76 (Activation)      (None, 8, 8, 48)     0           batch_normalization_76[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_76 (Conv2D)              (None, 8, 8, 12)     5184        activation_76[0][0]              \n","__________________________________________________________________________________________________\n","dropout_37 (Dropout)            (None, 8, 8, 12)     0           conv2d_76[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_37 (Concatenate)    (None, 8, 8, 258)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_77 (BatchNo (None, 8, 8, 258)    1032        concatenate_37[0][0]             \n","__________________________________________________________________________________________________\n","activation_77 (Activation)      (None, 8, 8, 258)    0           batch_normalization_77[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_77 (Conv2D)              (None, 8, 8, 48)     12384       activation_77[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_78 (BatchNo (None, 8, 8, 48)     192         conv2d_77[0][0]                  \n","__________________________________________________________________________________________________\n","activation_78 (Activation)      (None, 8, 8, 48)     0           batch_normalization_78[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_78 (Conv2D)              (None, 8, 8, 12)     5184        activation_78[0][0]              \n","__________________________________________________________________________________________________\n","dropout_38 (Dropout)            (None, 8, 8, 12)     0           conv2d_78[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_38 (Concatenate)    (None, 8, 8, 270)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_79 (BatchNo (None, 8, 8, 270)    1080        concatenate_38[0][0]             \n","__________________________________________________________________________________________________\n","activation_79 (Activation)      (None, 8, 8, 270)    0           batch_normalization_79[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_79 (Conv2D)              (None, 8, 8, 48)     12960       activation_79[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_80 (BatchNo (None, 8, 8, 48)     192         conv2d_79[0][0]                  \n","__________________________________________________________________________________________________\n","activation_80 (Activation)      (None, 8, 8, 48)     0           batch_normalization_80[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_80 (Conv2D)              (None, 8, 8, 12)     5184        activation_80[0][0]              \n","__________________________________________________________________________________________________\n","dropout_39 (Dropout)            (None, 8, 8, 12)     0           conv2d_80[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_39 (Concatenate)    (None, 8, 8, 282)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_81 (BatchNo (None, 8, 8, 282)    1128        concatenate_39[0][0]             \n","__________________________________________________________________________________________________\n","activation_81 (Activation)      (None, 8, 8, 282)    0           batch_normalization_81[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_81 (Conv2D)              (None, 8, 8, 48)     13536       activation_81[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_82 (BatchNo (None, 8, 8, 48)     192         conv2d_81[0][0]                  \n","__________________________________________________________________________________________________\n","activation_82 (Activation)      (None, 8, 8, 48)     0           batch_normalization_82[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_82 (Conv2D)              (None, 8, 8, 12)     5184        activation_82[0][0]              \n","__________________________________________________________________________________________________\n","dropout_40 (Dropout)            (None, 8, 8, 12)     0           conv2d_82[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_40 (Concatenate)    (None, 8, 8, 294)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","                                                                 dropout_40[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_83 (BatchNo (None, 8, 8, 294)    1176        concatenate_40[0][0]             \n","__________________________________________________________________________________________________\n","activation_83 (Activation)      (None, 8, 8, 294)    0           batch_normalization_83[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_83 (Conv2D)              (None, 8, 8, 48)     14112       activation_83[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_84 (BatchNo (None, 8, 8, 48)     192         conv2d_83[0][0]                  \n","__________________________________________________________________________________________________\n","activation_84 (Activation)      (None, 8, 8, 48)     0           batch_normalization_84[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_84 (Conv2D)              (None, 8, 8, 12)     5184        activation_84[0][0]              \n","__________________________________________________________________________________________________\n","dropout_41 (Dropout)            (None, 8, 8, 12)     0           conv2d_84[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_41 (Concatenate)    (None, 8, 8, 306)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","                                                                 dropout_40[0][0]                 \n","                                                                 dropout_41[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_85 (BatchNo (None, 8, 8, 306)    1224        concatenate_41[0][0]             \n","__________________________________________________________________________________________________\n","activation_85 (Activation)      (None, 8, 8, 306)    0           batch_normalization_85[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_85 (Conv2D)              (None, 8, 8, 48)     14688       activation_85[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_86 (BatchNo (None, 8, 8, 48)     192         conv2d_85[0][0]                  \n","__________________________________________________________________________________________________\n","activation_86 (Activation)      (None, 8, 8, 48)     0           batch_normalization_86[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_86 (Conv2D)              (None, 8, 8, 12)     5184        activation_86[0][0]              \n","__________________________________________________________________________________________________\n","dropout_42 (Dropout)            (None, 8, 8, 12)     0           conv2d_86[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_42 (Concatenate)    (None, 8, 8, 318)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","                                                                 dropout_40[0][0]                 \n","                                                                 dropout_41[0][0]                 \n","                                                                 dropout_42[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_87 (BatchNo (None, 8, 8, 318)    1272        concatenate_42[0][0]             \n","__________________________________________________________________________________________________\n","activation_87 (Activation)      (None, 8, 8, 318)    0           batch_normalization_87[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_87 (Conv2D)              (None, 8, 8, 48)     15264       activation_87[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_88 (BatchNo (None, 8, 8, 48)     192         conv2d_87[0][0]                  \n","__________________________________________________________________________________________________\n","activation_88 (Activation)      (None, 8, 8, 48)     0           batch_normalization_88[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_88 (Conv2D)              (None, 8, 8, 12)     5184        activation_88[0][0]              \n","__________________________________________________________________________________________________\n","dropout_43 (Dropout)            (None, 8, 8, 12)     0           conv2d_88[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_43 (Concatenate)    (None, 8, 8, 330)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","                                                                 dropout_40[0][0]                 \n","                                                                 dropout_41[0][0]                 \n","                                                                 dropout_42[0][0]                 \n","                                                                 dropout_43[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_89 (BatchNo (None, 8, 8, 330)    1320        concatenate_43[0][0]             \n","__________________________________________________________________________________________________\n","activation_89 (Activation)      (None, 8, 8, 330)    0           batch_normalization_89[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_89 (Conv2D)              (None, 8, 8, 48)     15840       activation_89[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_90 (BatchNo (None, 8, 8, 48)     192         conv2d_89[0][0]                  \n","__________________________________________________________________________________________________\n","activation_90 (Activation)      (None, 8, 8, 48)     0           batch_normalization_90[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_90 (Conv2D)              (None, 8, 8, 12)     5184        activation_90[0][0]              \n","__________________________________________________________________________________________________\n","dropout_44 (Dropout)            (None, 8, 8, 12)     0           conv2d_90[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_44 (Concatenate)    (None, 8, 8, 342)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","                                                                 dropout_40[0][0]                 \n","                                                                 dropout_41[0][0]                 \n","                                                                 dropout_42[0][0]                 \n","                                                                 dropout_43[0][0]                 \n","                                                                 dropout_44[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_91 (BatchNo (None, 8, 8, 342)    1368        concatenate_44[0][0]             \n","__________________________________________________________________________________________________\n","activation_91 (Activation)      (None, 8, 8, 342)    0           batch_normalization_91[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_91 (Conv2D)              (None, 8, 8, 48)     16416       activation_91[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_92 (BatchNo (None, 8, 8, 48)     192         conv2d_91[0][0]                  \n","__________________________________________________________________________________________________\n","activation_92 (Activation)      (None, 8, 8, 48)     0           batch_normalization_92[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_92 (Conv2D)              (None, 8, 8, 12)     5184        activation_92[0][0]              \n","__________________________________________________________________________________________________\n","dropout_45 (Dropout)            (None, 8, 8, 12)     0           conv2d_92[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_45 (Concatenate)    (None, 8, 8, 354)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","                                                                 dropout_40[0][0]                 \n","                                                                 dropout_41[0][0]                 \n","                                                                 dropout_42[0][0]                 \n","                                                                 dropout_43[0][0]                 \n","                                                                 dropout_44[0][0]                 \n","                                                                 dropout_45[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_93 (BatchNo (None, 8, 8, 354)    1416        concatenate_45[0][0]             \n","__________________________________________________________________________________________________\n","activation_93 (Activation)      (None, 8, 8, 354)    0           batch_normalization_93[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_93 (Conv2D)              (None, 8, 8, 48)     16992       activation_93[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_94 (BatchNo (None, 8, 8, 48)     192         conv2d_93[0][0]                  \n","__________________________________________________________________________________________________\n","activation_94 (Activation)      (None, 8, 8, 48)     0           batch_normalization_94[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_94 (Conv2D)              (None, 8, 8, 12)     5184        activation_94[0][0]              \n","__________________________________________________________________________________________________\n","dropout_46 (Dropout)            (None, 8, 8, 12)     0           conv2d_94[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_46 (Concatenate)    (None, 8, 8, 366)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","                                                                 dropout_40[0][0]                 \n","                                                                 dropout_41[0][0]                 \n","                                                                 dropout_42[0][0]                 \n","                                                                 dropout_43[0][0]                 \n","                                                                 dropout_44[0][0]                 \n","                                                                 dropout_45[0][0]                 \n","                                                                 dropout_46[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_95 (BatchNo (None, 8, 8, 366)    1464        concatenate_46[0][0]             \n","__________________________________________________________________________________________________\n","activation_95 (Activation)      (None, 8, 8, 366)    0           batch_normalization_95[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_95 (Conv2D)              (None, 8, 8, 48)     17568       activation_95[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_96 (BatchNo (None, 8, 8, 48)     192         conv2d_95[0][0]                  \n","__________________________________________________________________________________________________\n","activation_96 (Activation)      (None, 8, 8, 48)     0           batch_normalization_96[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_96 (Conv2D)              (None, 8, 8, 12)     5184        activation_96[0][0]              \n","__________________________________________________________________________________________________\n","dropout_47 (Dropout)            (None, 8, 8, 12)     0           conv2d_96[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_47 (Concatenate)    (None, 8, 8, 378)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","                                                                 dropout_40[0][0]                 \n","                                                                 dropout_41[0][0]                 \n","                                                                 dropout_42[0][0]                 \n","                                                                 dropout_43[0][0]                 \n","                                                                 dropout_44[0][0]                 \n","                                                                 dropout_45[0][0]                 \n","                                                                 dropout_46[0][0]                 \n","                                                                 dropout_47[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_97 (BatchNo (None, 8, 8, 378)    1512        concatenate_47[0][0]             \n","__________________________________________________________________________________________________\n","activation_97 (Activation)      (None, 8, 8, 378)    0           batch_normalization_97[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_97 (Conv2D)              (None, 8, 8, 48)     18144       activation_97[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_98 (BatchNo (None, 8, 8, 48)     192         conv2d_97[0][0]                  \n","__________________________________________________________________________________________________\n","activation_98 (Activation)      (None, 8, 8, 48)     0           batch_normalization_98[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_98 (Conv2D)              (None, 8, 8, 12)     5184        activation_98[0][0]              \n","__________________________________________________________________________________________________\n","dropout_48 (Dropout)            (None, 8, 8, 12)     0           conv2d_98[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_48 (Concatenate)    (None, 8, 8, 390)    0           average_pooling2d_2[0][0]        \n","                                                                 dropout_33[0][0]                 \n","                                                                 dropout_34[0][0]                 \n","                                                                 dropout_35[0][0]                 \n","                                                                 dropout_36[0][0]                 \n","                                                                 dropout_37[0][0]                 \n","                                                                 dropout_38[0][0]                 \n","                                                                 dropout_39[0][0]                 \n","                                                                 dropout_40[0][0]                 \n","                                                                 dropout_41[0][0]                 \n","                                                                 dropout_42[0][0]                 \n","                                                                 dropout_43[0][0]                 \n","                                                                 dropout_44[0][0]                 \n","                                                                 dropout_45[0][0]                 \n","                                                                 dropout_46[0][0]                 \n","                                                                 dropout_47[0][0]                 \n","                                                                 dropout_48[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_99 (BatchNo (None, 8, 8, 390)    1560        concatenate_48[0][0]             \n","__________________________________________________________________________________________________\n","activation_99 (Activation)      (None, 8, 8, 390)    0           batch_normalization_99[0][0]     \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 390)          0           activation_99[0][0]              \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 10)           3910        global_average_pooling2d_1[0][0] \n","==================================================================================================\n","Total params: 840,658\n","Trainable params: 815,242\n","Non-trainable params: 25,416\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"1FJ7kxYeT0zz","colab":{},"outputId":"e7f04e49-0f43-472f-b7fb-eef0f166a48e"},"cell_type":"code","source":["#make keras use all availabel GPU's\n","parallel_model = multi_gpu_model(model, gpus=4)\n","\n","def step_decay(epoch):\n","  \"\"\"\n","  Function to change learning rate based on epochs\n","  \"\"\"\n","  initial_lrate = 0.1\n","  lrate = 0.1\n","  if epoch >= 50 and epoch < 150:\n","    lrate = initial_lrate / 10\n","  if epoch >= 150 and epoch < 225:\n","    lrate = initial_lrate / 100\n","  if epoch >= 225:\n","    lrate = initial_lrate / 1000\n","  return float(lrate)\n","\n","lrate = LearningRateScheduler(step_decay)\n","\n","#optimizer\n","opt =  SGD(lr=0.1,momentum=0.9)\n","\n","#compile model\n","parallel_model.compile(optimizer=opt,\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","#defining checkpoints\n","if not os.path.exists('DenseNetBC100_12'):\n","  os.mkdir('DenseNetBC100_12')\n","    \n","filepath = \"DenseNetBC100_12/DenseNetBC100-improvement-{epoch:04d}-{val_acc:.4f}.hdf5\"\n","save_best = ModelCheckpoint(filepath,\n","                            monitor='val_acc',\n","                            verbose=1, \n","                            save_best_only=True, \n","                            save_weights_only=False,\n","                            mode='auto',\n","                            period=1)\n","\n","#fit model\n","history = parallel_model.fit(x_train_a,\n","                    y_train_a_h,\n","                    batch_size=64,\n","                    epochs=250,\n","                    validation_data = (x_test, y_test_h),\n","                    callbacks=[lrate,save_best])\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 150000 samples, validate on 10000 samples\n","Epoch 1/250\n","150000/150000 [==============================] - 795s 5ms/step - loss: 2.1362 - acc: 0.5357 - val_loss: 2.1851 - val_acc: 0.5272\n","\n","Epoch 00001: val_acc improved from -inf to 0.52720, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0001-0.5272.hdf5\n","Epoch 2/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 1.1722 - acc: 0.7233 - val_loss: 1.1565 - val_acc: 0.7159\n","\n","Epoch 00002: val_acc improved from 0.52720 to 0.71590, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0002-0.7159.hdf5\n","Epoch 3/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.8466 - acc: 0.7768 - val_loss: 1.0299 - val_acc: 0.7256\n","\n","Epoch 00003: val_acc improved from 0.71590 to 0.72560, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0003-0.7256.hdf5\n","Epoch 4/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.7157 - acc: 0.8053 - val_loss: 0.7519 - val_acc: 0.7974\n","\n","Epoch 00004: val_acc improved from 0.72560 to 0.79740, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0004-0.7974.hdf5\n","Epoch 5/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.6558 - acc: 0.8196 - val_loss: 0.6607 - val_acc: 0.8339\n","\n","Epoch 00005: val_acc improved from 0.79740 to 0.83390, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0005-0.8339.hdf5\n","Epoch 6/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.6117 - acc: 0.8351 - val_loss: 0.6636 - val_acc: 0.8244\n","\n","Epoch 00006: val_acc did not improve from 0.83390\n","Epoch 7/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.5812 - acc: 0.8458 - val_loss: 0.5696 - val_acc: 0.8560\n","\n","Epoch 00007: val_acc improved from 0.83390 to 0.85600, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0007-0.8560.hdf5\n","Epoch 8/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.5602 - acc: 0.8532 - val_loss: 0.5654 - val_acc: 0.8591\n","\n","Epoch 00008: val_acc improved from 0.85600 to 0.85910, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0008-0.8591.hdf5\n","Epoch 9/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.5358 - acc: 0.8624 - val_loss: 0.5953 - val_acc: 0.8572\n","\n","Epoch 00009: val_acc did not improve from 0.85910\n","Epoch 10/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.5158 - acc: 0.8686 - val_loss: 0.5917 - val_acc: 0.8615\n","\n","Epoch 00010: val_acc improved from 0.85910 to 0.86150, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0010-0.8615.hdf5\n","Epoch 11/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.5019 - acc: 0.8734 - val_loss: 0.6468 - val_acc: 0.8385\n","\n","Epoch 00011: val_acc did not improve from 0.86150\n","Epoch 12/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.4879 - acc: 0.8783 - val_loss: 0.6110 - val_acc: 0.8475\n","\n","Epoch 00012: val_acc did not improve from 0.86150\n","Epoch 13/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.4756 - acc: 0.8830 - val_loss: 0.5915 - val_acc: 0.8533\n","\n","Epoch 00013: val_acc did not improve from 0.86150\n","Epoch 14/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.4660 - acc: 0.8877 - val_loss: 0.5428 - val_acc: 0.8707\n","\n","Epoch 00014: val_acc improved from 0.86150 to 0.87070, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0014-0.8707.hdf5\n","Epoch 15/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.4569 - acc: 0.8907 - val_loss: 0.6041 - val_acc: 0.8540\n","\n","Epoch 00015: val_acc did not improve from 0.87070\n","Epoch 16/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.4478 - acc: 0.8940 - val_loss: 0.5008 - val_acc: 0.8833\n","\n","Epoch 00016: val_acc improved from 0.87070 to 0.88330, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0016-0.8833.hdf5\n","Epoch 17/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.4386 - acc: 0.8973 - val_loss: 0.5192 - val_acc: 0.8824\n","\n","Epoch 00017: val_acc did not improve from 0.88330\n","Epoch 18/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.4291 - acc: 0.9003 - val_loss: 0.5611 - val_acc: 0.8691\n","\n","Epoch 00018: val_acc did not improve from 0.88330\n","Epoch 19/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.4265 - acc: 0.9011 - val_loss: 0.7000 - val_acc: 0.8416\n","\n","Epoch 00019: val_acc did not improve from 0.88330\n","Epoch 20/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.4181 - acc: 0.9041 - val_loss: 0.5390 - val_acc: 0.8777\n","\n","Epoch 00020: val_acc did not improve from 0.88330\n","Epoch 21/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.4143 - acc: 0.9058 - val_loss: 0.4940 - val_acc: 0.8942\n","\n","Epoch 00021: val_acc improved from 0.88330 to 0.89420, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0021-0.8942.hdf5\n","Epoch 22/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.4075 - acc: 0.9083 - val_loss: 0.5671 - val_acc: 0.8631\n","\n","Epoch 00022: val_acc did not improve from 0.89420\n","Epoch 23/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.4053 - acc: 0.9090 - val_loss: 0.5414 - val_acc: 0.8720\n","\n","Epoch 00023: val_acc did not improve from 0.89420\n","Epoch 24/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.4014 - acc: 0.9102 - val_loss: 0.5267 - val_acc: 0.8804\n","\n","Epoch 00024: val_acc did not improve from 0.89420\n","Epoch 25/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.3956 - acc: 0.9127 - val_loss: 0.5420 - val_acc: 0.8797\n","\n","Epoch 00025: val_acc did not improve from 0.89420\n","Epoch 26/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.3902 - acc: 0.9144 - val_loss: 0.6382 - val_acc: 0.8601\n","\n","Epoch 00026: val_acc did not improve from 0.89420\n","Epoch 27/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.3851 - acc: 0.9163 - val_loss: 0.4883 - val_acc: 0.8934\n","\n","Epoch 00027: val_acc did not improve from 0.89420\n","Epoch 28/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.3844 - acc: 0.9166 - val_loss: 0.5217 - val_acc: 0.8865\n","\n","Epoch 00028: val_acc did not improve from 0.89420\n","Epoch 29/250\n","150000/150000 [==============================] - 702s 5ms/step - loss: 0.3796 - acc: 0.9183 - val_loss: 0.4790 - val_acc: 0.8877\n","\n","Epoch 00029: val_acc did not improve from 0.89420\n","Epoch 30/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.3772 - acc: 0.9196 - val_loss: 0.4878 - val_acc: 0.8855\n","\n","Epoch 00030: val_acc did not improve from 0.89420\n","Epoch 31/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.3697 - acc: 0.9221 - val_loss: 0.6764 - val_acc: 0.8448\n","\n","Epoch 00031: val_acc did not improve from 0.89420\n","Epoch 32/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.3701 - acc: 0.9223 - val_loss: 0.6636 - val_acc: 0.8550\n","\n","Epoch 00032: val_acc did not improve from 0.89420\n","Epoch 33/250\n","150000/150000 [==============================] - 702s 5ms/step - loss: 0.3637 - acc: 0.9244 - val_loss: 0.5020 - val_acc: 0.8927\n","\n","Epoch 00033: val_acc did not improve from 0.89420\n","Epoch 34/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.3663 - acc: 0.9231 - val_loss: 0.5797 - val_acc: 0.8749\n","\n","Epoch 00034: val_acc did not improve from 0.89420\n","Epoch 35/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.3607 - acc: 0.9255 - val_loss: 0.5053 - val_acc: 0.8895\n","\n","Epoch 00035: val_acc did not improve from 0.89420\n","Epoch 36/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.3598 - acc: 0.9255 - val_loss: 0.5836 - val_acc: 0.8750\n","\n","Epoch 00036: val_acc did not improve from 0.89420\n","Epoch 37/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.3547 - acc: 0.9278 - val_loss: 0.5494 - val_acc: 0.8747\n","\n","Epoch 00037: val_acc did not improve from 0.89420\n","Epoch 38/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.3535 - acc: 0.9280 - val_loss: 0.5425 - val_acc: 0.8890\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Epoch 00038: val_acc did not improve from 0.89420\n","Epoch 39/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.3524 - acc: 0.9291 - val_loss: 0.5537 - val_acc: 0.8822\n","\n","Epoch 00039: val_acc did not improve from 0.89420\n","Epoch 40/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.3514 - acc: 0.9284 - val_loss: 0.5195 - val_acc: 0.8907\n","\n","Epoch 00040: val_acc did not improve from 0.89420\n","Epoch 41/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.3490 - acc: 0.9290 - val_loss: 0.5780 - val_acc: 0.8820\n","\n","Epoch 00041: val_acc did not improve from 0.89420\n","Epoch 42/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.3472 - acc: 0.9303 - val_loss: 0.5833 - val_acc: 0.8803\n","\n","Epoch 00042: val_acc did not improve from 0.89420\n","Epoch 43/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.3442 - acc: 0.9319 - val_loss: 0.4603 - val_acc: 0.8988\n","\n","Epoch 00043: val_acc improved from 0.89420 to 0.89880, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0043-0.8988.hdf5\n","Epoch 44/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.3439 - acc: 0.9319 - val_loss: 0.5433 - val_acc: 0.8827\n","\n","Epoch 00044: val_acc did not improve from 0.89880\n","Epoch 45/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.3439 - acc: 0.9308 - val_loss: 0.4758 - val_acc: 0.9009\n","\n","Epoch 00045: val_acc improved from 0.89880 to 0.90090, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0045-0.9009.hdf5\n","Epoch 46/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.3404 - acc: 0.9331 - val_loss: 0.5455 - val_acc: 0.8814\n","\n","Epoch 00046: val_acc did not improve from 0.90090\n","Epoch 47/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.3392 - acc: 0.9334 - val_loss: 0.5470 - val_acc: 0.8885\n","\n","Epoch 00047: val_acc did not improve from 0.90090\n","Epoch 48/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.3380 - acc: 0.9323 - val_loss: 0.5075 - val_acc: 0.8918\n","\n","Epoch 00048: val_acc did not improve from 0.90090\n","Epoch 49/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.3362 - acc: 0.9342 - val_loss: 0.4779 - val_acc: 0.9015\n","\n","Epoch 00049: val_acc improved from 0.90090 to 0.90150, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0049-0.9015.hdf5\n","Epoch 50/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.3368 - acc: 0.9340 - val_loss: 0.4664 - val_acc: 0.9064\n","\n","Epoch 00050: val_acc improved from 0.90150 to 0.90640, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0050-0.9064.hdf5\n","Epoch 51/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.2413 - acc: 0.9678 - val_loss: 0.3607 - val_acc: 0.9336\n","\n","Epoch 00051: val_acc improved from 0.90640 to 0.93360, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0051-0.9336.hdf5\n","Epoch 52/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.2089 - acc: 0.9754 - val_loss: 0.3497 - val_acc: 0.9369\n","\n","Epoch 00052: val_acc improved from 0.93360 to 0.93690, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0052-0.9369.hdf5\n","Epoch 53/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1931 - acc: 0.9784 - val_loss: 0.3423 - val_acc: 0.9384\n","\n","Epoch 00053: val_acc improved from 0.93690 to 0.93840, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0053-0.9384.hdf5\n","Epoch 54/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1823 - acc: 0.9791 - val_loss: 0.3414 - val_acc: 0.9362\n","\n","Epoch 00054: val_acc did not improve from 0.93840\n","Epoch 55/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1727 - acc: 0.9800 - val_loss: 0.3248 - val_acc: 0.9381\n","\n","Epoch 00055: val_acc did not improve from 0.93840\n","Epoch 56/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.1640 - acc: 0.9807 - val_loss: 0.3246 - val_acc: 0.9378\n","\n","Epoch 00056: val_acc did not improve from 0.93840\n","Epoch 57/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1569 - acc: 0.9816 - val_loss: 0.3237 - val_acc: 0.9366\n","\n","Epoch 00057: val_acc did not improve from 0.93840\n","Epoch 58/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1520 - acc: 0.9811 - val_loss: 0.3392 - val_acc: 0.9323\n","\n","Epoch 00058: val_acc did not improve from 0.93840\n","Epoch 59/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1467 - acc: 0.9814 - val_loss: 0.3266 - val_acc: 0.9344\n","\n","Epoch 00059: val_acc did not improve from 0.93840\n","Epoch 60/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.1412 - acc: 0.9822 - val_loss: 0.3272 - val_acc: 0.9343\n","\n","Epoch 00060: val_acc did not improve from 0.93840\n","Epoch 61/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1382 - acc: 0.9816 - val_loss: 0.3112 - val_acc: 0.9368\n","\n","Epoch 00061: val_acc did not improve from 0.93840\n","Epoch 62/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.1360 - acc: 0.9814 - val_loss: 0.3157 - val_acc: 0.9328\n","\n","Epoch 00062: val_acc did not improve from 0.93840\n","Epoch 63/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.1330 - acc: 0.9815 - val_loss: 0.3142 - val_acc: 0.9349\n","\n","Epoch 00063: val_acc did not improve from 0.93840\n","Epoch 64/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.1313 - acc: 0.9810 - val_loss: 0.3058 - val_acc: 0.9345\n","\n","Epoch 00064: val_acc did not improve from 0.93840\n","Epoch 65/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1306 - acc: 0.9807 - val_loss: 0.3213 - val_acc: 0.9341\n","\n","Epoch 00065: val_acc did not improve from 0.93840\n","Epoch 66/250\n","150000/150000 [==============================] - 696s 5ms/step - loss: 0.1290 - acc: 0.9803 - val_loss: 0.3150 - val_acc: 0.9354\n","\n","Epoch 00066: val_acc did not improve from 0.93840\n","Epoch 67/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.1265 - acc: 0.9807 - val_loss: 0.3193 - val_acc: 0.9319\n","\n","Epoch 00067: val_acc did not improve from 0.93840\n","Epoch 68/250\n","150000/150000 [==============================] - 696s 5ms/step - loss: 0.1266 - acc: 0.9805 - val_loss: 0.3057 - val_acc: 0.9346\n","\n","Epoch 00068: val_acc did not improve from 0.93840\n","Epoch 69/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1252 - acc: 0.9808 - val_loss: 0.3225 - val_acc: 0.9294\n","\n","Epoch 00069: val_acc did not improve from 0.93840\n","Epoch 70/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.1238 - acc: 0.9807 - val_loss: 0.3044 - val_acc: 0.9352\n","\n","Epoch 00070: val_acc did not improve from 0.93840\n","Epoch 71/250\n","150000/150000 [==============================] - 696s 5ms/step - loss: 0.1240 - acc: 0.9800 - val_loss: 0.3185 - val_acc: 0.9327\n","\n","Epoch 00071: val_acc did not improve from 0.93840\n","Epoch 72/250\n","150000/150000 [==============================] - 696s 5ms/step - loss: 0.1235 - acc: 0.9805 - val_loss: 0.3025 - val_acc: 0.9350\n","\n","Epoch 00072: val_acc did not improve from 0.93840\n","Epoch 73/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1222 - acc: 0.9808 - val_loss: 0.3010 - val_acc: 0.9339\n","\n","Epoch 00073: val_acc did not improve from 0.93840\n","Epoch 74/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1229 - acc: 0.9799 - val_loss: 0.3206 - val_acc: 0.9313\n","\n","Epoch 00074: val_acc did not improve from 0.93840\n","Epoch 75/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1229 - acc: 0.9805 - val_loss: 0.3097 - val_acc: 0.9309\n","\n","Epoch 00075: val_acc did not improve from 0.93840\n","Epoch 76/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1212 - acc: 0.9807 - val_loss: 0.3086 - val_acc: 0.9319\n","\n","Epoch 00076: val_acc did not improve from 0.93840\n","Epoch 77/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1215 - acc: 0.9801 - val_loss: 0.3114 - val_acc: 0.9324\n","\n","Epoch 00077: val_acc did not improve from 0.93840\n","Epoch 78/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1197 - acc: 0.9807 - val_loss: 0.3249 - val_acc: 0.9284\n","\n","Epoch 00078: val_acc did not improve from 0.93840\n","Epoch 79/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1186 - acc: 0.9812 - val_loss: 0.3435 - val_acc: 0.9258\n","\n","Epoch 00079: val_acc did not improve from 0.93840\n","Epoch 80/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1209 - acc: 0.9805 - val_loss: 0.3920 - val_acc: 0.9159\n","\n","Epoch 00080: val_acc did not improve from 0.93840\n","Epoch 81/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1191 - acc: 0.9806 - val_loss: 0.3377 - val_acc: 0.9266\n","\n","Epoch 00081: val_acc did not improve from 0.93840\n","Epoch 82/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1194 - acc: 0.9808 - val_loss: 0.3461 - val_acc: 0.9265\n","\n","Epoch 00082: val_acc did not improve from 0.93840\n","Epoch 83/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.1203 - acc: 0.9803 - val_loss: 0.3281 - val_acc: 0.9283\n","\n","Epoch 00083: val_acc did not improve from 0.93840\n","Epoch 84/250\n","150000/150000 [==============================] - 696s 5ms/step - loss: 0.1188 - acc: 0.9811 - val_loss: 0.3163 - val_acc: 0.9321\n","\n","Epoch 00084: val_acc did not improve from 0.93840\n","Epoch 85/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.1176 - acc: 0.9813 - val_loss: 0.3294 - val_acc: 0.9295\n","\n","Epoch 00085: val_acc did not improve from 0.93840\n","Epoch 86/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.1167 - acc: 0.9818 - val_loss: 0.3226 - val_acc: 0.9305\n","\n","Epoch 00086: val_acc did not improve from 0.93840\n","Epoch 87/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1190 - acc: 0.9805 - val_loss: 0.3070 - val_acc: 0.9321\n","\n","Epoch 00087: val_acc did not improve from 0.93840\n","Epoch 88/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1171 - acc: 0.9815 - val_loss: 0.3353 - val_acc: 0.9286\n","\n","Epoch 00088: val_acc did not improve from 0.93840\n","Epoch 89/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1149 - acc: 0.9822 - val_loss: 0.3294 - val_acc: 0.9307\n","\n","Epoch 00089: val_acc did not improve from 0.93840\n","Epoch 90/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1138 - acc: 0.9828 - val_loss: 0.2915 - val_acc: 0.9362\n","\n","Epoch 00090: val_acc did not improve from 0.93840\n","Epoch 91/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1159 - acc: 0.9816 - val_loss: 0.3122 - val_acc: 0.9315\n","\n","Epoch 00091: val_acc did not improve from 0.93840\n","Epoch 92/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1152 - acc: 0.9823 - val_loss: 0.3179 - val_acc: 0.9341\n","\n","Epoch 00092: val_acc did not improve from 0.93840\n","Epoch 93/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1139 - acc: 0.9822 - val_loss: 0.3206 - val_acc: 0.9311\n","\n","Epoch 00093: val_acc did not improve from 0.93840\n","Epoch 94/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1163 - acc: 0.9813 - val_loss: 0.3185 - val_acc: 0.9340\n","\n","Epoch 00094: val_acc did not improve from 0.93840\n","Epoch 95/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1145 - acc: 0.9819 - val_loss: 0.3236 - val_acc: 0.9341\n","\n","Epoch 00095: val_acc did not improve from 0.93840\n","Epoch 96/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1124 - acc: 0.9833 - val_loss: 0.3433 - val_acc: 0.9265\n","\n","Epoch 00096: val_acc did not improve from 0.93840\n","Epoch 97/250\n","150000/150000 [==============================] - 702s 5ms/step - loss: 0.1131 - acc: 0.9821 - val_loss: 0.3468 - val_acc: 0.9280\n","\n","Epoch 00097: val_acc did not improve from 0.93840\n","Epoch 98/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1144 - acc: 0.9820 - val_loss: 0.3195 - val_acc: 0.9287\n","\n","Epoch 00098: val_acc did not improve from 0.93840\n","Epoch 99/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1159 - acc: 0.9816 - val_loss: 0.3269 - val_acc: 0.9290\n","\n","Epoch 00099: val_acc did not improve from 0.93840\n","Epoch 100/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1116 - acc: 0.9831 - val_loss: 0.3300 - val_acc: 0.9293\n","\n","Epoch 00100: val_acc did not improve from 0.93840\n","Epoch 101/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1135 - acc: 0.9823 - val_loss: 0.3058 - val_acc: 0.9344\n","\n","Epoch 00101: val_acc did not improve from 0.93840\n","Epoch 102/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1139 - acc: 0.9824 - val_loss: 0.3356 - val_acc: 0.9290\n","\n","Epoch 00102: val_acc did not improve from 0.93840\n","Epoch 103/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1125 - acc: 0.9827 - val_loss: 0.3190 - val_acc: 0.9314\n","\n","Epoch 00103: val_acc did not improve from 0.93840\n","Epoch 104/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1115 - acc: 0.9832 - val_loss: 0.3126 - val_acc: 0.9335\n","\n","Epoch 00104: val_acc did not improve from 0.93840\n","Epoch 105/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1126 - acc: 0.9826 - val_loss: 0.3228 - val_acc: 0.9311\n","\n","Epoch 00105: val_acc did not improve from 0.93840\n","Epoch 106/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1111 - acc: 0.9831 - val_loss: 0.3160 - val_acc: 0.9325\n","\n","Epoch 00106: val_acc did not improve from 0.93840\n","Epoch 107/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1117 - acc: 0.9830 - val_loss: 0.3133 - val_acc: 0.9317\n","\n","Epoch 00107: val_acc did not improve from 0.93840\n","Epoch 108/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1110 - acc: 0.9834 - val_loss: 0.3020 - val_acc: 0.9350\n","\n","Epoch 00108: val_acc did not improve from 0.93840\n","Epoch 109/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1114 - acc: 0.9832 - val_loss: 0.3191 - val_acc: 0.9300\n","\n","Epoch 00109: val_acc did not improve from 0.93840\n","Epoch 110/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1114 - acc: 0.9829 - val_loss: 0.3256 - val_acc: 0.9290\n","\n","Epoch 00110: val_acc did not improve from 0.93840\n","Epoch 111/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1110 - acc: 0.9834 - val_loss: 0.3253 - val_acc: 0.9304\n","\n","Epoch 00111: val_acc did not improve from 0.93840\n","Epoch 112/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1092 - acc: 0.9836 - val_loss: 0.3398 - val_acc: 0.9287\n","\n","Epoch 00112: val_acc did not improve from 0.93840\n","Epoch 113/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1105 - acc: 0.9835 - val_loss: 0.3012 - val_acc: 0.9345\n","\n","Epoch 00113: val_acc did not improve from 0.93840\n","Epoch 114/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.1097 - acc: 0.9838 - val_loss: 0.3470 - val_acc: 0.9252\n","\n","Epoch 00114: val_acc did not improve from 0.93840\n","Epoch 115/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1093 - acc: 0.9837 - val_loss: 0.3361 - val_acc: 0.9312\n","\n","Epoch 00115: val_acc did not improve from 0.93840\n","Epoch 116/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1082 - acc: 0.9840 - val_loss: 0.3232 - val_acc: 0.9319\n","\n","Epoch 00116: val_acc did not improve from 0.93840\n","Epoch 117/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1083 - acc: 0.9842 - val_loss: 0.3336 - val_acc: 0.9270\n","\n","Epoch 00117: val_acc did not improve from 0.93840\n","Epoch 118/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1079 - acc: 0.9841 - val_loss: 0.3254 - val_acc: 0.9320\n","\n","Epoch 00118: val_acc did not improve from 0.93840\n","Epoch 119/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1089 - acc: 0.9839 - val_loss: 0.3433 - val_acc: 0.9294\n","\n","Epoch 00119: val_acc did not improve from 0.93840\n","Epoch 120/250\n"],"name":"stdout"},{"output_type":"stream","text":["150000/150000 [==============================] - 698s 5ms/step - loss: 0.1083 - acc: 0.9839 - val_loss: 0.3365 - val_acc: 0.9296\n","\n","Epoch 00120: val_acc did not improve from 0.93840\n","Epoch 121/250\n","150000/150000 [==============================] - 705s 5ms/step - loss: 0.1082 - acc: 0.9840 - val_loss: 0.3312 - val_acc: 0.9321\n","\n","Epoch 00121: val_acc did not improve from 0.93840\n","Epoch 122/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1069 - acc: 0.9847 - val_loss: 0.3085 - val_acc: 0.9354\n","\n","Epoch 00122: val_acc did not improve from 0.93840\n","Epoch 123/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1070 - acc: 0.9845 - val_loss: 0.3274 - val_acc: 0.9332\n","\n","Epoch 00123: val_acc did not improve from 0.93840\n","Epoch 124/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1087 - acc: 0.9839 - val_loss: 0.3260 - val_acc: 0.9310\n","\n","Epoch 00124: val_acc did not improve from 0.93840\n","Epoch 125/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1079 - acc: 0.9842 - val_loss: 0.3228 - val_acc: 0.9346\n","\n","Epoch 00125: val_acc did not improve from 0.93840\n","Epoch 126/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1054 - acc: 0.9854 - val_loss: 0.3330 - val_acc: 0.9296\n","\n","Epoch 00126: val_acc did not improve from 0.93840\n","Epoch 127/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1069 - acc: 0.9847 - val_loss: 0.3635 - val_acc: 0.9278\n","\n","Epoch 00127: val_acc did not improve from 0.93840\n","Epoch 128/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1044 - acc: 0.9854 - val_loss: 0.3043 - val_acc: 0.9350\n","\n","Epoch 00128: val_acc did not improve from 0.93840\n","Epoch 129/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1061 - acc: 0.9844 - val_loss: 0.3435 - val_acc: 0.9269\n","\n","Epoch 00129: val_acc did not improve from 0.93840\n","Epoch 130/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1061 - acc: 0.9844 - val_loss: 0.3229 - val_acc: 0.9338\n","\n","Epoch 00130: val_acc did not improve from 0.93840\n","Epoch 131/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1063 - acc: 0.9844 - val_loss: 0.3312 - val_acc: 0.9272\n","\n","Epoch 00131: val_acc did not improve from 0.93840\n","Epoch 132/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1055 - acc: 0.9849 - val_loss: 0.3390 - val_acc: 0.9287\n","\n","Epoch 00132: val_acc did not improve from 0.93840\n","Epoch 133/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1059 - acc: 0.9848 - val_loss: 0.3169 - val_acc: 0.9302\n","\n","Epoch 00133: val_acc did not improve from 0.93840\n","Epoch 134/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1047 - acc: 0.9849 - val_loss: 0.3188 - val_acc: 0.9318\n","\n","Epoch 00134: val_acc did not improve from 0.93840\n","Epoch 135/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1048 - acc: 0.9854 - val_loss: 0.3126 - val_acc: 0.9344\n","\n","Epoch 00135: val_acc did not improve from 0.93840\n","Epoch 136/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1044 - acc: 0.9854 - val_loss: 0.3219 - val_acc: 0.9326\n","\n","Epoch 00136: val_acc did not improve from 0.93840\n","Epoch 137/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1050 - acc: 0.9848 - val_loss: 0.3128 - val_acc: 0.9309\n","\n","Epoch 00137: val_acc did not improve from 0.93840\n","Epoch 138/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1038 - acc: 0.9857 - val_loss: 0.3048 - val_acc: 0.9349\n","\n","Epoch 00138: val_acc did not improve from 0.93840\n","Epoch 139/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1028 - acc: 0.9859 - val_loss: 0.3204 - val_acc: 0.9321\n","\n","Epoch 00139: val_acc did not improve from 0.93840\n","Epoch 140/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1040 - acc: 0.9852 - val_loss: 0.3594 - val_acc: 0.9280\n","\n","Epoch 00140: val_acc did not improve from 0.93840\n","Epoch 141/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1034 - acc: 0.9856 - val_loss: 0.3145 - val_acc: 0.9360\n","\n","Epoch 00141: val_acc did not improve from 0.93840\n","Epoch 142/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.1048 - acc: 0.9851 - val_loss: 0.3151 - val_acc: 0.9325\n","\n","Epoch 00142: val_acc did not improve from 0.93840\n","Epoch 143/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1038 - acc: 0.9853 - val_loss: 0.3343 - val_acc: 0.9290\n","\n","Epoch 00143: val_acc did not improve from 0.93840\n","Epoch 144/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1055 - acc: 0.9848 - val_loss: 0.3136 - val_acc: 0.9333\n","\n","Epoch 00144: val_acc did not improve from 0.93840\n","Epoch 145/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1041 - acc: 0.9849 - val_loss: 0.3195 - val_acc: 0.9335\n","\n","Epoch 00145: val_acc did not improve from 0.93840\n","Epoch 146/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1030 - acc: 0.9855 - val_loss: 0.3439 - val_acc: 0.9263\n","\n","Epoch 00146: val_acc did not improve from 0.93840\n","Epoch 147/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1043 - acc: 0.9852 - val_loss: 0.3214 - val_acc: 0.9324\n","\n","Epoch 00147: val_acc did not improve from 0.93840\n","Epoch 148/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1035 - acc: 0.9856 - val_loss: 0.3233 - val_acc: 0.9319\n","\n","Epoch 00148: val_acc did not improve from 0.93840\n","Epoch 149/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.1024 - acc: 0.9860 - val_loss: 0.3228 - val_acc: 0.9317\n","\n","Epoch 00149: val_acc did not improve from 0.93840\n","Epoch 150/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.1035 - acc: 0.9854 - val_loss: 0.3515 - val_acc: 0.9267\n","\n","Epoch 00150: val_acc did not improve from 0.93840\n","Epoch 151/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0899 - acc: 0.9907 - val_loss: 0.3017 - val_acc: 0.9371\n","\n","Epoch 00151: val_acc did not improve from 0.93840\n","Epoch 152/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0834 - acc: 0.9929 - val_loss: 0.2964 - val_acc: 0.9394\n","\n","Epoch 00152: val_acc improved from 0.93840 to 0.93940, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0152-0.9394.hdf5\n","Epoch 153/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0818 - acc: 0.9934 - val_loss: 0.2953 - val_acc: 0.9402\n","\n","Epoch 00153: val_acc improved from 0.93940 to 0.94020, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0153-0.9402.hdf5\n","Epoch 154/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0810 - acc: 0.9936 - val_loss: 0.2956 - val_acc: 0.9409\n","\n","Epoch 00154: val_acc improved from 0.94020 to 0.94090, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0154-0.9409.hdf5\n","Epoch 155/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0808 - acc: 0.9933 - val_loss: 0.2975 - val_acc: 0.9410\n","\n","Epoch 00155: val_acc improved from 0.94090 to 0.94100, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0155-0.9410.hdf5\n","Epoch 156/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.0790 - acc: 0.9937 - val_loss: 0.2949 - val_acc: 0.9411\n","\n","Epoch 00156: val_acc improved from 0.94100 to 0.94110, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0156-0.9411.hdf5\n","Epoch 157/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0790 - acc: 0.9937 - val_loss: 0.2931 - val_acc: 0.9409\n","\n","Epoch 00157: val_acc did not improve from 0.94110\n","Epoch 158/250\n","150000/150000 [==============================] - 702s 5ms/step - loss: 0.0786 - acc: 0.9938 - val_loss: 0.2946 - val_acc: 0.9410\n","\n","Epoch 00158: val_acc did not improve from 0.94110\n","Epoch 159/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0775 - acc: 0.9942 - val_loss: 0.2950 - val_acc: 0.9415\n","\n","Epoch 00159: val_acc improved from 0.94110 to 0.94150, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0159-0.9415.hdf5\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 160/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0771 - acc: 0.9942 - val_loss: 0.2902 - val_acc: 0.9421\n","\n","Epoch 00160: val_acc improved from 0.94150 to 0.94210, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0160-0.9421.hdf5\n","Epoch 161/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0772 - acc: 0.9942 - val_loss: 0.2930 - val_acc: 0.9406\n","\n","Epoch 00161: val_acc did not improve from 0.94210\n","Epoch 162/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0767 - acc: 0.9943 - val_loss: 0.2912 - val_acc: 0.9406\n","\n","Epoch 00162: val_acc did not improve from 0.94210\n","Epoch 163/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0760 - acc: 0.9943 - val_loss: 0.2938 - val_acc: 0.9393\n","\n","Epoch 00163: val_acc did not improve from 0.94210\n","Epoch 164/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0758 - acc: 0.9943 - val_loss: 0.2944 - val_acc: 0.9406\n","\n","Epoch 00164: val_acc did not improve from 0.94210\n","Epoch 165/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0750 - acc: 0.9944 - val_loss: 0.2955 - val_acc: 0.9409\n","\n","Epoch 00165: val_acc did not improve from 0.94210\n","Epoch 166/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0749 - acc: 0.9943 - val_loss: 0.2928 - val_acc: 0.9408\n","\n","Epoch 00166: val_acc did not improve from 0.94210\n","Epoch 167/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.0747 - acc: 0.9944 - val_loss: 0.2923 - val_acc: 0.9411\n","\n","Epoch 00167: val_acc did not improve from 0.94210\n","Epoch 168/250\n","150000/150000 [==============================] - 696s 5ms/step - loss: 0.0738 - acc: 0.9948 - val_loss: 0.2949 - val_acc: 0.9406\n","\n","Epoch 00168: val_acc did not improve from 0.94210\n","Epoch 169/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.0744 - acc: 0.9943 - val_loss: 0.2918 - val_acc: 0.9405\n","\n","Epoch 00169: val_acc did not improve from 0.94210\n","Epoch 170/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.0736 - acc: 0.9947 - val_loss: 0.2927 - val_acc: 0.9427\n","\n","Epoch 00170: val_acc improved from 0.94210 to 0.94270, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0170-0.9427.hdf5\n","Epoch 171/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.0735 - acc: 0.9946 - val_loss: 0.2931 - val_acc: 0.9415\n","\n","Epoch 00171: val_acc did not improve from 0.94270\n","Epoch 172/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0728 - acc: 0.9946 - val_loss: 0.2952 - val_acc: 0.9411\n","\n","Epoch 00172: val_acc did not improve from 0.94270\n","Epoch 173/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0727 - acc: 0.9947 - val_loss: 0.2939 - val_acc: 0.9420\n","\n","Epoch 00173: val_acc did not improve from 0.94270\n","Epoch 174/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0726 - acc: 0.9945 - val_loss: 0.2924 - val_acc: 0.9412\n","\n","Epoch 00174: val_acc did not improve from 0.94270\n","Epoch 175/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0723 - acc: 0.9945 - val_loss: 0.2939 - val_acc: 0.9414\n","\n","Epoch 00175: val_acc did not improve from 0.94270\n","Epoch 176/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0723 - acc: 0.9944 - val_loss: 0.2936 - val_acc: 0.9414\n","\n","Epoch 00176: val_acc did not improve from 0.94270\n","Epoch 177/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0716 - acc: 0.9948 - val_loss: 0.2894 - val_acc: 0.9427\n","\n","Epoch 00177: val_acc did not improve from 0.94270\n","Epoch 178/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0714 - acc: 0.9948 - val_loss: 0.2950 - val_acc: 0.9405\n","\n","Epoch 00178: val_acc did not improve from 0.94270\n","Epoch 179/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0716 - acc: 0.9945 - val_loss: 0.2924 - val_acc: 0.9410\n","\n","Epoch 00179: val_acc did not improve from 0.94270\n","Epoch 180/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0712 - acc: 0.9945 - val_loss: 0.2942 - val_acc: 0.9404\n","\n","Epoch 00180: val_acc did not improve from 0.94270\n","Epoch 181/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0703 - acc: 0.9949 - val_loss: 0.2891 - val_acc: 0.9419\n","\n","Epoch 00181: val_acc did not improve from 0.94270\n","Epoch 182/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0704 - acc: 0.9949 - val_loss: 0.2910 - val_acc: 0.9410\n","\n","Epoch 00182: val_acc did not improve from 0.94270\n","Epoch 183/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0701 - acc: 0.9947 - val_loss: 0.2891 - val_acc: 0.9411\n","\n","Epoch 00183: val_acc did not improve from 0.94270\n","Epoch 184/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0701 - acc: 0.9946 - val_loss: 0.2984 - val_acc: 0.9404\n","\n","Epoch 00184: val_acc did not improve from 0.94270\n","Epoch 185/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0696 - acc: 0.9947 - val_loss: 0.2894 - val_acc: 0.9421\n","\n","Epoch 00185: val_acc did not improve from 0.94270\n","Epoch 186/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0697 - acc: 0.9946 - val_loss: 0.2909 - val_acc: 0.9431\n","\n","Epoch 00186: val_acc improved from 0.94270 to 0.94310, saving model to DenseNetBC100_12/DenseNetBC100-improvement-0186-0.9431.hdf5\n","Epoch 187/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0690 - acc: 0.9949 - val_loss: 0.2882 - val_acc: 0.9423\n","\n","Epoch 00187: val_acc did not improve from 0.94310\n","Epoch 188/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0689 - acc: 0.9947 - val_loss: 0.2939 - val_acc: 0.9403\n","\n","Epoch 00188: val_acc did not improve from 0.94310\n","Epoch 189/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0690 - acc: 0.9945 - val_loss: 0.2920 - val_acc: 0.9413\n","\n","Epoch 00189: val_acc did not improve from 0.94310\n","Epoch 190/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0695 - acc: 0.9945 - val_loss: 0.2887 - val_acc: 0.9424\n","\n","Epoch 00190: val_acc did not improve from 0.94310\n","Epoch 191/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0680 - acc: 0.9952 - val_loss: 0.2949 - val_acc: 0.9408\n","\n","Epoch 00191: val_acc did not improve from 0.94310\n","Epoch 192/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0684 - acc: 0.9946 - val_loss: 0.2946 - val_acc: 0.9414\n","\n","Epoch 00192: val_acc did not improve from 0.94310\n","Epoch 193/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.0680 - acc: 0.9947 - val_loss: 0.2887 - val_acc: 0.9412\n","\n","Epoch 00193: val_acc did not improve from 0.94310\n","Epoch 194/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.0677 - acc: 0.9949 - val_loss: 0.2907 - val_acc: 0.9426\n","\n","Epoch 00194: val_acc did not improve from 0.94310\n","Epoch 195/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0677 - acc: 0.9949 - val_loss: 0.2929 - val_acc: 0.9408\n","\n","Epoch 00195: val_acc did not improve from 0.94310\n","Epoch 196/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0681 - acc: 0.9945 - val_loss: 0.2911 - val_acc: 0.9421\n","\n","Epoch 00196: val_acc did not improve from 0.94310\n","Epoch 197/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0677 - acc: 0.9945 - val_loss: 0.2970 - val_acc: 0.9397\n","\n","Epoch 00197: val_acc did not improve from 0.94310\n","Epoch 198/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0674 - acc: 0.9948 - val_loss: 0.2910 - val_acc: 0.9404\n","\n","Epoch 00198: val_acc did not improve from 0.94310\n","Epoch 199/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0669 - acc: 0.9948 - val_loss: 0.2899 - val_acc: 0.9422\n","\n","Epoch 00199: val_acc did not improve from 0.94310\n","Epoch 200/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0668 - acc: 0.9948 - val_loss: 0.2873 - val_acc: 0.9418\n","\n","Epoch 00200: val_acc did not improve from 0.94310\n","Epoch 201/250\n"],"name":"stdout"},{"output_type":"stream","text":["150000/150000 [==============================] - 702s 5ms/step - loss: 0.0670 - acc: 0.9949 - val_loss: 0.2932 - val_acc: 0.9413\n","\n","Epoch 00201: val_acc did not improve from 0.94310\n","Epoch 202/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0675 - acc: 0.9941 - val_loss: 0.2928 - val_acc: 0.9418\n","\n","Epoch 00202: val_acc did not improve from 0.94310\n","Epoch 203/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0663 - acc: 0.9947 - val_loss: 0.2918 - val_acc: 0.9409\n","\n","Epoch 00203: val_acc did not improve from 0.94310\n","Epoch 204/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0659 - acc: 0.9946 - val_loss: 0.2900 - val_acc: 0.9410\n","\n","Epoch 00204: val_acc did not improve from 0.94310\n","Epoch 205/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.0661 - acc: 0.9946 - val_loss: 0.2910 - val_acc: 0.9410\n","\n","Epoch 00205: val_acc did not improve from 0.94310\n","Epoch 206/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0660 - acc: 0.9945 - val_loss: 0.2905 - val_acc: 0.9416\n","\n","Epoch 00206: val_acc did not improve from 0.94310\n","Epoch 207/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0650 - acc: 0.9951 - val_loss: 0.2909 - val_acc: 0.9423\n","\n","Epoch 00207: val_acc did not improve from 0.94310\n","Epoch 208/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0656 - acc: 0.9946 - val_loss: 0.2864 - val_acc: 0.9418\n","\n","Epoch 00208: val_acc did not improve from 0.94310\n","Epoch 209/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0654 - acc: 0.9948 - val_loss: 0.2875 - val_acc: 0.9421\n","\n","Epoch 00209: val_acc did not improve from 0.94310\n","Epoch 210/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0652 - acc: 0.9949 - val_loss: 0.2926 - val_acc: 0.9396\n","\n","Epoch 00210: val_acc did not improve from 0.94310\n","Epoch 211/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0647 - acc: 0.9948 - val_loss: 0.2909 - val_acc: 0.9410\n","\n","Epoch 00211: val_acc did not improve from 0.94310\n","Epoch 212/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0651 - acc: 0.9948 - val_loss: 0.2934 - val_acc: 0.9397\n","\n","Epoch 00212: val_acc did not improve from 0.94310\n","Epoch 213/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0648 - acc: 0.9946 - val_loss: 0.2922 - val_acc: 0.9408\n","\n","Epoch 00213: val_acc did not improve from 0.94310\n","Epoch 214/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0643 - acc: 0.9946 - val_loss: 0.2915 - val_acc: 0.9408\n","\n","Epoch 00214: val_acc did not improve from 0.94310\n","Epoch 215/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0641 - acc: 0.9951 - val_loss: 0.2914 - val_acc: 0.9410\n","\n","Epoch 00215: val_acc did not improve from 0.94310\n","Epoch 216/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0642 - acc: 0.9948 - val_loss: 0.2926 - val_acc: 0.9406\n","\n","Epoch 00216: val_acc did not improve from 0.94310\n","Epoch 217/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0652 - acc: 0.9943 - val_loss: 0.2889 - val_acc: 0.9402\n","\n","Epoch 00217: val_acc did not improve from 0.94310\n","Epoch 218/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0643 - acc: 0.9947 - val_loss: 0.2915 - val_acc: 0.9410\n","\n","Epoch 00218: val_acc did not improve from 0.94310\n","Epoch 219/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0639 - acc: 0.9947 - val_loss: 0.2938 - val_acc: 0.9401\n","\n","Epoch 00219: val_acc did not improve from 0.94310\n","Epoch 220/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0638 - acc: 0.9948 - val_loss: 0.2928 - val_acc: 0.9403\n","\n","Epoch 00220: val_acc did not improve from 0.94310\n","Epoch 221/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0634 - acc: 0.9949 - val_loss: 0.2935 - val_acc: 0.9408\n","\n","Epoch 00221: val_acc did not improve from 0.94310\n","Epoch 222/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0634 - acc: 0.9948 - val_loss: 0.2912 - val_acc: 0.9402\n","\n","Epoch 00222: val_acc did not improve from 0.94310\n","Epoch 223/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.0632 - acc: 0.9947 - val_loss: 0.2922 - val_acc: 0.9398\n","\n","Epoch 00223: val_acc did not improve from 0.94310\n","Epoch 224/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.0639 - acc: 0.9945 - val_loss: 0.2912 - val_acc: 0.9417\n","\n","Epoch 00224: val_acc did not improve from 0.94310\n","Epoch 225/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0641 - acc: 0.9943 - val_loss: 0.2887 - val_acc: 0.9405\n","\n","Epoch 00225: val_acc did not improve from 0.94310\n","Epoch 226/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.0624 - acc: 0.9950 - val_loss: 0.2910 - val_acc: 0.9403\n","\n","Epoch 00226: val_acc did not improve from 0.94310\n","Epoch 227/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.0620 - acc: 0.9950 - val_loss: 0.2884 - val_acc: 0.9407\n","\n","Epoch 00227: val_acc did not improve from 0.94310\n","Epoch 228/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0622 - acc: 0.9953 - val_loss: 0.2891 - val_acc: 0.9414\n","\n","Epoch 00228: val_acc did not improve from 0.94310\n","Epoch 229/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0619 - acc: 0.9952 - val_loss: 0.2888 - val_acc: 0.9417\n","\n","Epoch 00229: val_acc did not improve from 0.94310\n","Epoch 230/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0619 - acc: 0.9950 - val_loss: 0.2882 - val_acc: 0.9414\n","\n","Epoch 00230: val_acc did not improve from 0.94310\n","Epoch 231/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0614 - acc: 0.9953 - val_loss: 0.2871 - val_acc: 0.9413\n","\n","Epoch 00231: val_acc did not improve from 0.94310\n","Epoch 232/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0610 - acc: 0.9954 - val_loss: 0.2868 - val_acc: 0.9420\n","\n","Epoch 00232: val_acc did not improve from 0.94310\n","Epoch 233/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0612 - acc: 0.9954 - val_loss: 0.2856 - val_acc: 0.9417\n","\n","Epoch 00233: val_acc did not improve from 0.94310\n","Epoch 234/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0615 - acc: 0.9952 - val_loss: 0.2856 - val_acc: 0.9419\n","\n","Epoch 00234: val_acc did not improve from 0.94310\n","Epoch 235/250\n","150000/150000 [==============================] - 697s 5ms/step - loss: 0.0613 - acc: 0.9951 - val_loss: 0.2852 - val_acc: 0.9415\n","\n","Epoch 00235: val_acc did not improve from 0.94310\n","Epoch 236/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0607 - acc: 0.9954 - val_loss: 0.2847 - val_acc: 0.9424\n","\n","Epoch 00236: val_acc did not improve from 0.94310\n","Epoch 237/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0611 - acc: 0.9955 - val_loss: 0.2857 - val_acc: 0.9419\n","\n","Epoch 00237: val_acc did not improve from 0.94310\n","Epoch 238/250\n","150000/150000 [==============================] - 698s 5ms/step - loss: 0.0608 - acc: 0.9954 - val_loss: 0.2867 - val_acc: 0.9421\n","\n","Epoch 00238: val_acc did not improve from 0.94310\n","Epoch 239/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0604 - acc: 0.9956 - val_loss: 0.2859 - val_acc: 0.9421\n","\n","Epoch 00239: val_acc did not improve from 0.94310\n","Epoch 240/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0606 - acc: 0.9955 - val_loss: 0.2859 - val_acc: 0.9412\n","\n","Epoch 00240: val_acc did not improve from 0.94310\n","Epoch 241/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.0613 - acc: 0.9952 - val_loss: 0.2879 - val_acc: 0.9408\n","\n","Epoch 00241: val_acc did not improve from 0.94310\n","Epoch 242/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0605 - acc: 0.9958 - val_loss: 0.2861 - val_acc: 0.9410\n","\n","Epoch 00242: val_acc did not improve from 0.94310\n","Epoch 243/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0613 - acc: 0.9950 - val_loss: 0.2872 - val_acc: 0.9417\n","\n","Epoch 00243: val_acc did not improve from 0.94310\n","Epoch 244/250\n","150000/150000 [==============================] - 700s 5ms/step - loss: 0.0607 - acc: 0.9954 - val_loss: 0.2858 - val_acc: 0.9416\n","\n","Epoch 00244: val_acc did not improve from 0.94310\n","Epoch 245/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.0609 - acc: 0.9955 - val_loss: 0.2861 - val_acc: 0.9420\n","\n","Epoch 00245: val_acc did not improve from 0.94310\n","Epoch 246/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0610 - acc: 0.9952 - val_loss: 0.2855 - val_acc: 0.9413\n","\n","Epoch 00246: val_acc did not improve from 0.94310\n","Epoch 247/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0609 - acc: 0.9953 - val_loss: 0.2854 - val_acc: 0.9410\n","\n","Epoch 00247: val_acc did not improve from 0.94310\n","Epoch 248/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0606 - acc: 0.9953 - val_loss: 0.2857 - val_acc: 0.9414\n","\n","Epoch 00248: val_acc did not improve from 0.94310\n","Epoch 249/250\n","150000/150000 [==============================] - 701s 5ms/step - loss: 0.0611 - acc: 0.9952 - val_loss: 0.2852 - val_acc: 0.9423\n","\n","Epoch 00249: val_acc did not improve from 0.94310\n","Epoch 250/250\n","150000/150000 [==============================] - 699s 5ms/step - loss: 0.0601 - acc: 0.9957 - val_loss: 0.2843 - val_acc: 0.9414\n","\n","Epoch 00250: val_acc did not improve from 0.94310\n"],"name":"stdout"}]},{"metadata":{"id":"mZUf9pTGNaPV","colab_type":"text"},"cell_type":"markdown","source":["### Evaluate model for test score"]},{"metadata":{"colab_type":"code","id":"uCAyUlcBmfZ6","colab":{},"outputId":"17cc7f65-0868-4cce-e086-5fbe572fa4c3"},"cell_type":"code","source":["score = parallel_model.evaluate(x_test, y_test_h)\n","print('Test accuracy:{}\\nTest loss:{}'.format(score[1], score[0]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 30s 3ms/step\n","Test accuracy:0.9414\n","Test loss:0.28429777430295944\n"],"name":"stdout"}]},{"metadata":{"id":"nSMGIK16NaPX","colab_type":"text"},"cell_type":"markdown","source":["### Best test accuracy reached"]},{"metadata":{"id":"1Unnd2AfNaPZ","colab_type":"code","colab":{},"outputId":"641b343d-b6f0-4516-afd6-b6b4ebdddba4"},"cell_type":"code","source":["#create model layers\n","model = create_densenet_network()\n","parallel_model = multi_gpu_model(model, gpus=4)\n","\n","#load best weights\n","parallel_model.load_weights('DenseNetBC100_12/DenseNetBC100-improvement-0186-0.9431.hdf5')\n","\n","#compile model\n","opt =  SGD(lr=0.1,momentum=0.9)\n","\n","#compile model\n","parallel_model.compile(optimizer=opt,\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","#evaluate model\n","score = parallel_model.evaluate(x_test, y_test_h)\n","print('Best test accuracy:{}\\nBest test loss:{}'.format(score[1],score[0]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 61s 6ms/step\n","Best test accuracy:0.9431\n","Best test loss:0.29094115722179414\n"],"name":"stdout"}]},{"metadata":{"id":"qgKxIfHYNaPb","colab_type":"text"},"cell_type":"markdown","source":["### Plots of Model's loss and accuracy over 250 epochs"]},{"metadata":{"id":"cKBvFn2VNaPb","colab_type":"code","colab":{}},"cell_type":"code","source":["import seaborn as sb\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6dgx-sc1NaPd","colab_type":"text"},"cell_type":"markdown","source":["#### Train and validation accuracy"]},{"metadata":{"id":"O3P2OaLVNaPe","colab_type":"code","colab":{},"outputId":"ea43850a-89f5-4e08-8634-83abf13cafa0"},"cell_type":"code","source":["sb.set_style(\"darkgrid\")\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'] )\n","plt.show()\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"buXPc7dcNaPi","colab_type":"text"},"cell_type":"markdown","source":["#### Train and validation loss"]},{"metadata":{"id":"J6qcmzw0NaPi","colab_type":"code","colab":{},"outputId":"e39e92dc-f6bd-4989-d523-b39aecab6864"},"cell_type":"code","source":["sb.set_style(\"darkgrid\")\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'])\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"dZYJ3oJqNaPk","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}